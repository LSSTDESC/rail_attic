{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAIL Evaluation - Checking results against DC1 paper\n",
    "\n",
    "The purpose of this notebook is to validate the reimplementation of the DC1 metrics, previously available on Github repository [PZDC1paper](https://github.com/LSSTDESC/PZDC1paper), now refactored to be part of RAIL Evaluation module. The metrics here were implemented in object-oriented Python 3, following a superclass/subclass structure, and inheriting features from _qp_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "from utils import *\n",
    "import os\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DC1 results\n",
    "The DC1 results are stored in the class `DC1` (defined in `utils.py` ancillary file), which exists only to provide the reference values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc1 = DC1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc1.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access individual metrics, one can call the metrics dictionary `dc1.results` using the codes and metrics names as keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc1.results['PIT out rate']['FlexZBoost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of codes and metrics available can be accessed by the properties `dc1.codes` and `dc1.metrics`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dc1.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dc1.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "<a class=\"anchor\" id=\"data\"></a>\n",
    "## The data   \n",
    "\n",
    " In this notebook we use the same input dataset used in DC1 PZ paper ([Schmidt et al. 2020](https://arxiv.org/pdf/2001.03621.pdf)), copied from cori (/global/cfs/cdirs/lsst/groups/PZ/PhotoZDC1/photoz_results/TESTDC1FLEXZ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = \"/Users/julia/TESTDC1FLEXZ\"\n",
    "\n",
    "pdfs_file =  os.path.join(my_path, \"Mar5Flexzgold_pz.out\")\n",
    "ztrue_file =  os.path.join(my_path, \"Mar5Flexzgold_idszmag.out\")\n",
    "oldpitfile = os.path.join(my_path,\"TESTPITVALS.out\")\n",
    "pdfs, zgrid, ztrue, photoz_mode = read_pz_output(pdfs_file, ztrue_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Metrics calculated based on the PITs computed via qp.Ensemble CDF method. The PIT values can be passed as optional input to speed up the metrics calculation. If no PIT array is provided, it is calculated on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pit = PIT(pdfs, zgrid, ztrue)\n",
    "pit.evaluate()\n",
    "pits = pit.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = Summary(pdfs, zgrid, ztrue)\n",
    "summary.markdown_metrics_table(pits=pits, show_dc1=\"FlexZBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PIT-QQ plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_out_rate = PitOutRate(pdfs, zgrid, ztrue).evaluate(pits=pits)\n",
    "pit.plot_pit_qq(title=\"DC1 paper data\", code=\"FlexZBoost\",\n",
    "                pit_out_rate=pit_out_rate, savefig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### \"Debugging\"\n",
    "\n",
    "Following Sam's suggestion, I also computed the metrics reading the PIT values from the partial results of DC1 paper, instead of calculating them from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading DC1 PIT values (PITs computed in the past for the paper):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pits_dc1 = np.loadtxt(oldpitfile, skiprows=1,usecols=(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,3])\n",
    "plt.plot(pits_dc1, pits-pits_dc1, 'k,')\n",
    "plt.plot([0,1], [0,0], 'r--', lw=3)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(-0.02, 0.02)\n",
    "plt.xlabel(\"PITs from DC1 paper\")\n",
    "plt.ylabel(\"$\\Delta$ PIT\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are slightly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalculating the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = Summary(pdfs, zgrid, ztrue)\n",
    "summary.markdown_metrics_table(pits=pits_dc1, show_dc1=\"FlexZBoost\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_out_rate = PitOutRate(pdfs, zgrid, ztrue).evaluate(pits=pits_dc1)\n",
    "pit.plot_pit_qq(title=\"DC1 data (original PITs)\", code=\"FlexZBoost\",\n",
    "                pit_out_rate=pit_out_rate, savefig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the original PIT values from the paper, all metrics match reasonably, except for the Anderson-Darling statistic. \n",
    "\n",
    "#### Anderson-Darling \n",
    "\n",
    "$$ \\mathrm{AD}^2 \\equiv N_{tot} \\int_{-\\infty}^{\\infty} \\frac{\\big( \\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\big)^{2}}{\\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\big( 1 \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\big)}\\mathrm{dCDF}(\\tilde{f}, z) $$ \n",
    "\n",
    "\n",
    "The class AD uses `scipy.stats.anderson_ksamp` method to compute the Anderson-Darling statistic for the PIT values by comparing with a uniform distribution between 0 and 1. Up to the current version (1.6.2), `scipy.stats.anderson` (the 1-sample test) does not support uniform distributions as reference sample.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson_ksamp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_dc1 = dc1.results['AD']['FlexZBoost']\n",
    "ad_dc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = AD(pdfs, zgrid, ztrue)\n",
    "ad.evaluate(pits=pits_dc1)\n",
    "ad.metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the AD is computed within the interval $0.0 \\leq PIT \\leq 1.0$. \n",
    "\n",
    "5 objects have PIT values out of this interval (this is unexpected). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pits_dc1[(pits_dc1<0)|(pits_dc1>1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to remove extreme values of PIT, as done in the paper.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.evaluate(pits=pits, ad_pit_min=0.0001, ad_pit_max=0.9999)\n",
    "ad.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.evaluate(pits=pits, ad_pit_min=0.001, ad_pit_max=0.999)\n",
    "ad.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.evaluate(pits=pits, ad_pit_min=0.01, ad_pit_max=0.99)\n",
    "ad.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_err = (abs(ad_dc1-ad.metric)/ad_dc1)*100. # percent error\n",
    "print(f\"Percent error: {p_err:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "#### Point estimates metrics\n",
    "\n",
    "These metrics are deprecated and might not be used in future analyses. They are included in this notebook for the sake of reproducing the results from the paper in its totality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.old_metrics_table(photoz_mode, ztrue, name=\"this test\", show_dc1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_old_valid(photoz_mode, ztrue, code=\"FlexZBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics calculated using the new implementation are reasonably close to the expected values. Minor differences were observed due to the difference in the calculation of PIT values. In both cases, here and in the paper, the PITs were calculated using _qp_ functions. The small diferences are attributed to minor changes in _qp_ versions since when the paper was produced. \n",
    "\n",
    "When using the original values of PIT, i.e., those calculated for the paper using the _qp_ version availabe at the time, all metrics were reproduced, except for the AD test. This particular metric is quite sensitive to the range of PITs considered in the calculation. Using the same PIT interval as used in the paper (0.01,0.99), the result obtained using the new implementation diverges from the paper result by 19.3%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
