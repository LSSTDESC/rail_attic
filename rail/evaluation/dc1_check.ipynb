{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAIL Evaluation - Check results against DC1 paper\n",
    "\n",
    "Contact: _Julia Gschwend_ ([julia@linea.gov.br](mailto:julia@linea.gov.br)), _Sam Schmidt, Alex Malz, Eric Charles_\n",
    "\n",
    "The purpose of this notebook is to validate the new implementation of the DC1 metrics, previously available on Github repository [PZDC1paper](https://github.com/LSSTDESC/PZDC1paper), now refactored to be part of RAIL Evaluation module. The metrics here were implemented in object-oriented Python 3, inheriting features from _qp_. In this notebook we use the same input dataset used in DC1 PZ paper ([Schmidt et al. 2020](https://arxiv.org/pdf/2001.03621.pdf)), copied from cori (/global/cfs/cdirs/lsst/groups/PZ/PhotoZDC1/photoz_results/TESTDC1FLEXZ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from sample import Sample\n",
    "from metrics import *\n",
    "import utils\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"sample\"></a>\n",
    "\n",
    "## Sample  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = \"/Users/julia/TESTDC1FLEXZ\"\n",
    "\n",
    "pdfs_file =  os.path.join(my_path, \"Mar5Flexzgold_pz.out\")\n",
    "ztrue_file =  os.path.join(my_path, \"Mar5Flexzgold_idszmag.out\")\n",
    "\n",
    "#pdfs_file =  os.path.join(my_path, \"1pct_Mar5Flexzgold_pz.out\")\n",
    "#ztrue_file =  os.path.join(my_path, \"1pct_Mar5Flexzgold_idszmag.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample = Sample(pdfs_file, ztrue_file, code=\"FlexZBoost\", name=\"DC1 paper data\")\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics = Metrics(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics below are based on the PIT and the CDF(PIT), both computed via qp.Ensemble object method. The PIT array is computed as the qp.Ensemble CDF function for an object containing the photo-z PDFs, evaluated at the true $z$ for each galaxy. The PIT distribution is implemented as the normalized histogram of PIT values. The uniform U(0,1) is implemented as a mock normalized distribution with the same number of bins of PIT distribution, where all values are equal to $1/N_{quant}$.     \n",
    "Then a new qp.Ensemble object is instantiated for each distribution, PITs and U(0,1), to use the CDF functionallity (an ensemble with only 1 PDF each)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "class Metrics:\n",
    "    \"\"\"\n",
    "       ***   Metrics class   ***\n",
    "    Receives a Sample object as input.\n",
    "    Computes PIT and QQ vectors on the initialization.\n",
    "    It's the basis for the other metrics, such as KS, AD, and CvM.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample, n_quant=100, pit_min=0.0001, pit_max=0.9999, debug=False):\n",
    "        \"\"\"Class constructor\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample: `Sample`\n",
    "            sample object defined in ./sample.py\n",
    "        n_quant: `int`, (optional)\n",
    "            number of quantiles for the QQ plot\n",
    "        pit_min: `float`\n",
    "            lower limit to define PIT outliers\n",
    "            default is 0.0001\n",
    "        pit_max:\n",
    "            upper limit to define PIT outliers\n",
    "            default is 0.9999\n",
    "        \"\"\"\n",
    "        self._sample = sample\n",
    "        self._n_quant = n_quant\n",
    "        self._pit_min = pit_min\n",
    "        self._pit_max = pit_max\n",
    "        self._debug = debug\n",
    "        n = len(self._sample)\n",
    "        if debug:\n",
    "            #n = 1000 # subset for quick tests\n",
    "            print(\"DEBUG MODE\")\n",
    "            #ids = np.random.choice(n, 10000)\n",
    "            self._pit = np.loadtxt(os.path.join(sample.path,\"TESTPITVALS.out\"), unpack=True, usecols=[1])#[ids]\n",
    "            self.new_pit = np.nan_to_num([self._sample._pdfs[i].cdf(self._sample._ztrue[i])[0][0] for i in range(n)])# ids])\n",
    "        else:\n",
    "            n = len(self._sample)\n",
    "            self._pit = np.nan_to_num([self._sample._pdfs[i].cdf(self._sample._ztrue[i])[0][0] for i in range(n)])\n",
    "        # Quantiles\n",
    "        Qtheory = np.linspace(0., 1., self.n_quant)\n",
    "        Qdata = np.quantile(self._pit, Qtheory)\n",
    "        self._qq_vectors = (Qtheory, Qdata)\n",
    "        # Normalized distribution of PIT values (PIT PDF)\n",
    "        self._xvals = Qtheory\n",
    "        self._pit_pdf, self._pit_bins_edges = np.histogram(self._pit, bins=n_quant, density=True)\n",
    "        #self._uniform_pdf = stats.uniform(self._xvals, scale=n_quant)\n",
    "        self._uniform_pdf = np.full(n_quant, 1.0 / float(n_quant))\n",
    "        # Define qp Ensemble to use CDF functionality (an ensemble with only 1 PDF)\n",
    "        self._pit_ensemble = qp.Ensemble(qp.hist, data=dict(bins=self._pit_bins_edges,\n",
    "                                                            pdfs=np.array([self._pit_pdf])))\n",
    "        self._uniform_ensemble = qp.Ensemble(qp.interp, data=dict(xvals=self._xvals,\n",
    "                                                                  yvals=np.array([self._uniform_pdf])))\n",
    "        self._pit_cdf = self._pit_ensemble.cdf(self._xvals)[0]\n",
    "        self._uniform_cdf = self._uniform_ensemble.cdf(self._xvals)[0]\n",
    "        \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PIT-QQ plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_pit_qq() #savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DC1 results\n",
    "The DC1 results are stored in Metrics class object as a table and as a dictionary, inheriting from an independent class DC1 (in `utils.py` ancillary file), which exists only to provide the reference values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.dc1.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.dc1.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.dc1.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.dc1.results['PIT out rate']['FlexZBoost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary table with all metrics containing DC1 paper results for comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.markdown_metrics_table(show_dc1=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first attempt, the results do not match, except for the PIT outliers rate. The CDE loss is close to the reference values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = abs(metrics.cde_loss - metrics.dc1.results['CDE loss']['FlexZBoost'])\n",
    "perc = abs(delta/metrics.dc1.results['CDE loss']['FlexZBoost'])*100.\n",
    "print(f\"CDE loss differs from DC1 value by {delta:.3f} ({perc:.1f}%).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such small difference could be explained by differences in the binning used for the numerical integration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "However, the KS, CvM, and AD tests still need to be fixed.\n",
    "Let's investigate these numbers by comparing the results with what we would get if using the scipy built-in statistical tests (implemented as alternative methods for each metric). \n",
    "\n",
    "### Kolmogorov-Smirnov  \n",
    "\n",
    "$$\n",
    "\\mathrm{KS} \\equiv \\max_{PIT} \\Big( \\left| \\ \\mathrm{CDF} \\small[ \\hat{f}, z \\small] - \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\  \\right| \\Big)\n",
    "$$\n",
    "\n",
    "```python\n",
    "    def __init__(self, metrics, scipy=False):\n",
    "        self._metrics = metrics\n",
    "        if scipy:\n",
    "            self._stat, self._pvalue = stats.kstest(metrics._pit, \"uniform\")\n",
    "        else:\n",
    "            self._stat, self._pvalue = np.max(np.abs(metrics._pit_cdf - metrics._uniform_cdf)), None # p=value TBD\n",
    "        # update Metrics object\n",
    "        metrics._ks_stat = self._stat\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_dc1 = metrics.dc1.results['KS']['FlexZBoost']\n",
    "ks_dc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = KS(metrics)\n",
    "ks.stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_sci = KS(metrics, scipy=True)\n",
    "ks_sci.stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Komolgorof-Smirnov test, the values with and without using scipy.stats.ks_test function are compatible with each other and both disagree with the DC1 result significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = abs(ks_sci.stat - metrics.dc1.results['KS']['FlexZBoost'])\n",
    "perc = abs(delta/metrics.dc1.results['KS']['FlexZBoost'])*100.\n",
    "print(f\"KS differs from DC1 value by {delta:.3f} ({perc:.1f}%).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual interpretation of KS test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_sci.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> SOLUTION STILL PENDING!!!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cramer-von Mises\n",
    "\n",
    "Let's fepeat the same excercise with the CvM test.\n",
    "\n",
    "$$ \\mathrm{CvM}^2 \\equiv \\int_{-\\infty}^{\\infty} \\Big( \\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\Big)^{2} \\mathrm{dCDF}(\\tilde{f}, z) $$ \n",
    "\n",
    "```python\n",
    "\n",
    "    def __init__(self, metrics, scipy=False):\n",
    "        if scipy:\n",
    "            cvm_result = stats.cramervonmises(metrics._pit_dist, \"uniform\")\n",
    "            self._stat, self._pvalue = cvm_result.statistic, cvm_result.pvalue\n",
    "        else:\n",
    "            self._stat, self._pvalue = np.sqrt(np.trapz((metrics._pit_cdf - metrics._uniform_cdf)**2, metrics._uniform_cdf)), None # p-value TBD\n",
    "        # update Metrics object\n",
    "        metrics._cvm_stat = self._stat\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm_dc1 = metrics.dc1.results['CvM']['FlexZBoost']\n",
    "cvm_dc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm = CvM(metrics)\n",
    "cvm.stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm_sci = CvM(metrics, scipy=True)\n",
    "cvm_sci.stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, all numbers disagree. I have checked the code fr CvM test in `skgof` library, and it doesn't look like the equation for the definition of CvM shown in the paper.\n",
    "\n",
    "From https://github.com/wrwrwr/scikit-gof/blob/master/skgof/ecdfgof.py: \n",
    "\n",
    "```python\n",
    "\n",
    "def cvm_stat(data):\n",
    "    \"\"\"\n",
    "    Calculates the Cramer-von Mises statistic for sorted values from U(0, 1).\n",
    "    \"\"\"\n",
    "    samples2 = 2 * len(data)\n",
    "    minuends = arange(1, samples2, 2) / samples2\n",
    "    return 1 / (6 * samples2) + ((minuends - data) ** 2).sum()\n",
    "\n",
    "(...)\n",
    "\n",
    "cvm_test = partial(simple_test, stat=cvm_stat, pdist=cvm_unif)\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> SOLUTION STILL PENDING!!!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anderson-Darling \n",
    "\n",
    "The last matric is the AD test, which is the onluy metric that allows the removal of extreme outliers before the calculation:\n",
    "\n",
    "$$ \\mathrm{AD}^2 \\equiv N_{tot} \\int_{-\\infty}^{\\infty} \\frac{\\big( \\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\big)^{2}}{\\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\big( 1 \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\big)}\\mathrm{dCDF}(\\tilde{f}, z) $$ \n",
    "\n",
    "```python\n",
    "    def __init__(self, metrics, ad_pit_min=0.0, ad_pit_max=1.0):\n",
    "\n",
    "        mask_pit = (metrics._pit >= ad_pit_min) & (metrics._pit  <= ad_pit_max)\n",
    "        if (ad_pit_min != 0.0) or (ad_pit_max != 1.0):\n",
    "            n_out = len(metrics._pit) - len(metrics._pit[mask_pit])\n",
    "            perc_out = (float(n_out)/float(len(metrics._pit)))*100.\n",
    "            print(f\"{n_out} outliers (PIT<{ad_pit_min} or PIT>{ad_pit_max}) removed from the calculation ({perc_out:.1f}%)\")\n",
    "\n",
    "        ad_xvals = np.linspace(ad_pit_min, ad_pit_max, metrics.n_quant)\n",
    "        ad_yscale_uniform = (ad_pit_max-ad_pit_min)/float(metrics._n_quant)\n",
    "        ad_pit_dist, ad_pit_bins_edges = np.histogram(metrics.pit[mask_pit], bins=metrics.n_quant, density=True)\n",
    "        ad_uniform_dist = np.full(metrics.n_quant, ad_yscale_uniform)\n",
    "        # Redo CDFs to account for outliers mask\n",
    "        ad_pit_ensemble = qp.Ensemble(qp.hist, data=dict(bins=ad_pit_bins_edges, pdfs=np.array([ad_pit_dist])))\n",
    "        ad_pit_cdf = ad_pit_ensemble.cdf(ad_xvals)[0]\n",
    "        ad_uniform_ensemble = qp.Ensemble(qp.hist,\n",
    "                                          data=dict(bins=ad_pit_bins_edges, pdfs=np.array([ad_uniform_dist])))\n",
    "        ad_uniform_cdf = ad_uniform_ensemble.cdf(ad_xvals)[0]\n",
    "        numerator = ((ad_pit_cdf - ad_uniform_cdf)**2)\n",
    "        denominator = (ad_uniform_cdf*(1.-ad_uniform_cdf))\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            self._stat = np.sqrt(float(len(metrics._sample)) * np.trapz(np.nan_to_num(numerator/denominator), ad_uniform_cdf))\n",
    "        # update Metrics object\n",
    "        metrics._ad_stat = self._stat\n",
    "```\n",
    "\n",
    "For the Anderson-Darling test, the comparison to a uniform distribution is not available in scipy.stats.anderson method, so using it does not make sense. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_dc1 = metrics.dc1.results['AD']['FlexZBoost']\n",
    "ad_dc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = AD(metrics).stat\n",
    "ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the catastrophic autliers (as done in the paper), to see the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_clean = AD(metrics, ad_pit_min=0.01, ad_pit_max=0.99).stat\n",
    "ad_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more, the results disagree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> SOLUTION STILL PENDING!!!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "\n",
    "Following Sam's suggestion, I also computed the metrics reading the PIT values from the partial results of DC1 paper, instead of calculating them in advance. The \"debug\" mode of `metrics` class uses DC1's PIT values. This mode will probably be removed of the code after solving all bugs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics_debug = Metrics(sample, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the comment section of RAIL's  [pull request #54](https://github.com/LSSTDESC/RAIL/pull/54), Sam pointed out the small disagreement found between the PIT values of DC1 sample computed now (using current `qp` version), and those computed at the time of the paper writing. There is a trend or new values of PIT to be slightly larger than the old for PIT < 0.5 and slightly smaller for PIT > 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,3])\n",
    "plt.plot(metrics_debug.pit, metrics.pit - metrics_debug.pit, 'k,')\n",
    "plt.plot([0,1], [0,0], 'r--', lw=3)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(-0.015, 0.015)\n",
    "plt.xlabel(\"PITs from DC1 paper\")\n",
    "plt.ylabel(\"$\\Delta$ PIT\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results using DC1's PIT values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_debug.markdown_metrics_table(show_dc1=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the `scipy=True` version of the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_debug_sci = KS(metrics_debug, scipy=True)\n",
    "ks_debug_sci.stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_dc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm_debug_sci = CvM(metrics_debug, scipy=True)\n",
    "cvm_debug_sci.stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm_dc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> SOLUTION STILL PENDING!!!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point estimates metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_metrics_table = sample.plot_old_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.old_metrics_table(sample, show_dc1=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least the point metrics agree, so the PDFs are being read correctely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> I still need help to understand the disagreement in the results. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
