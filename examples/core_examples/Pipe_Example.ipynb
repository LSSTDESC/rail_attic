{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAIL Pipeline example notebook\n",
    "\n",
    "author: Eric Charles<br>\n",
    "last run successfully: April 26, 2023<br>\n",
    "\n",
    "This notbook shows how to:\n",
    "1. Build a simple rail pipeline interactive, \n",
    "2. Save that pipeline (including configuraiton) to a yaml file, \n",
    "3. Load that pipeline from the saved yaml file,\n",
    "4. Run the loaded pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import ceci\n",
    "import rail\n",
    "from rail.core.stage import RailStage\n",
    "from rail.creation.degradation import LSSTErrorModel, InvRedshiftIncompleteness, LineConfusion, QuantityCut\n",
    "from rail.creation.engines.flowEngine import FlowCreator, FlowPosterior\n",
    "from rail.core.data import TableHandle\n",
    "from rail.core.stage import RailStage\n",
    "from rail.core.utilStages import ColumnMapper, TableConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by setting up the Rail data store. RAIL uses [ceci](https://github.com/LSSTDESC/ceci), which is designed for pipelines rather than interactive notebooks; the data store will work around that and enable us to use data interactively.\n",
    "When working interactively, we want to allow overwriting data in the Rail data store to avoid errors if we re-run cells. \n",
    "See the `rail/examples/goldenspike/goldenspike.ipynb` example notebook for more details on the Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some configuration setup\n",
    "\n",
    "The example pipeline builds some of the RAIL creation functionality into a pipeline. \n",
    "\n",
    "Here we are defining:\n",
    "1. The location of the pretrained PZFlow file used with this example.\n",
    "2. The bands we will be generating data for.\n",
    "3. The names of the columns where we will be writing the error estimates.\n",
    "4. The grid of redshifts we use for posterior estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.core.utils import RAILDIR\n",
    "flow_file = os.path.join(RAILDIR, 'rail/examples_data/goldenspike_data/data/pretrained_flow.pkl')\n",
    "bands = ['u','g','r','i','z','y']\n",
    "band_dict = {band:f'mag_{band}_lsst' for band in bands}\n",
    "rename_dict = {f'mag_{band}_lsst_err':f'mag_err_{band}_lsst' for band in bands}\n",
    "post_grid = [float(x) for x in np.linspace(0., 5, 21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pipeline stages\n",
    "\n",
    "The RailStage base class defines the `make_stage` \"classmethod\" function, which allows us to make a stage of\n",
    "that particular type in a general way.\n",
    "\n",
    "Note that that we are passing in the configuration parameters to each pipeline stage as keyword arguments.\n",
    "\n",
    "The names of the parameters will depend on the stage type.\n",
    "\n",
    "A couple of things are important:\n",
    "1. Each stage should have a unique name. In `ceci`, stage names default to the name of the class (e.g., FlowCreator, or LSSTErrorModel); this would be problematic if you wanted two stages of the same type in a given pipeline, so be sure to assign each stage its own name.\n",
    "2. At this point, we aren't actually worrying about the connections between the stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_engine_test = FlowCreator.make_stage(name='flow_engine_test', \n",
    "                                         model=flow_file, n_samples=50)\n",
    "      \n",
    "lsst_error_model_test = LSSTErrorModel.make_stage(name='lsst_error_model_test',\n",
    "                                                  bandNames=band_dict)\n",
    "                \n",
    "col_remapper_test = ColumnMapper.make_stage(name='col_remapper_test', hdf5_groupname='',\n",
    "                                            columns=rename_dict)\n",
    "\n",
    "flow_post_test = FlowPosterior.make_stage(name='flow_post_test',\n",
    "                                          column='redshift', flow=flow_file,\n",
    "                                          grid=post_grid)\n",
    "\n",
    "table_conv_test = TableConverter.make_stage(name='table_conv_test', output_format='numpyDict', \n",
    "                                            seed=12345)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the pipeline and add the stages\n",
    "\n",
    "Here we make an empty interactive pipeline (interactive in the sense that it will be run locally, rather than using the batch submission mechanisms built into `ceci`), and add the stages to that pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ceci.Pipeline.interactive()\n",
    "stages = [flow_engine_test, lsst_error_model_test, col_remapper_test, table_conv_test]\n",
    "for stage in stages:\n",
    "    pipe.add_stage(stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are some examples of interactive introspection into the pipeline\n",
    "\n",
    "I.e., some functions that you can use to figure out what the pipeline is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the stages\n",
    "pipe.stage_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the configuration of a particular stage\n",
    "pipe.flow_engine_test.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of outputs 'tags' \n",
    "# These are how the stage thinks of the outputs, as a list names associated to DataHandle types.\n",
    "pipe.flow_engine_test.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of outputs 'aliased tags'\n",
    "# These are how the pipeline things of the outputs, as a unique key that points to a particular file\n",
    "pipe.flow_engine_test._outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, now let's connect up the pipeline stages\n",
    "\n",
    "We can use the `RailStage.connect_input` function to connect one stage to another.\n",
    "By default, this will connect the output data product called `output` for one stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsst_error_model_test.connect_input(flow_engine_test)\n",
    "col_remapper_test.connect_input(lsst_error_model_test)\n",
    "#flow_post_test.connect_input(col_remapper_test, inputTag='input')\n",
    "table_conv_test.connect_input(col_remapper_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the pipeline\n",
    "\n",
    "This will do a few things:\n",
    "1. Attach any global pipeline inputs that were not specified in the connections above. In our case, the input flow file is pre-existing and must be specified as a global input.\n",
    "2. Specifiy output and logging directories.\n",
    "3. Optionally, create the pipeline in 'resume' mode, where it will ignore stages if all of their output already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.initialize(dict(model=flow_file), dict(output_dir='.', log_dir='.', resume=False), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the pipeline\n",
    "\n",
    "This will actually write two files (b/c this is what `ceci` wants)\n",
    "\n",
    "1. pipe_example.yml, which will have a list of stages, with instructions on how to execute the stages (e.g., run this stage in parallel on 20 processors). For an interactive pipeline, those instructions will be trivial.\n",
    "2. pipe_example_config.yml, which will have a dictionary of configurations for each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.save('pipe_saved.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the saved pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = ceci.Pipeline.read('pipe_saved.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the newly read pipeline\n",
    "\n",
    "This will actually launch Unix process to individually run each stage of the pipeline; you can see the commands that are being executed in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "1cc5b75da6b94fd59f6c7b0992047078a9372d2242c3f23a554e39af5a039534"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
