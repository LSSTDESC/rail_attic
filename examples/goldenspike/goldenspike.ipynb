{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "449c685e",
   "metadata": {},
   "source": [
    "# Goldenspike, an example of an end-to-end analysis using RAIL\n",
    "\n",
    "This notebook demonstrates how to use a the various RAIL Modules to draw synthetic samples of fluxes by color, apply physical effects to them, train photo-Z estimators on the samples, test and validate the preformance of those estimators, and to use the RAIL summarization modules to obtain n(z) estimates based on the p(z) estimates.\n",
    "\n",
    "### Creation \n",
    "\n",
    "Note that in the parlance of the Creation Module, \"degradation\" is any post-processing that occurs to the \"true\" sample generated by the create Engine.  This can include adding photometric errors, applying quality cuts, introducing systematic biases, etc.\n",
    "\n",
    "In this notebook, we will draw both test and training samples from a RAIL Engine object. Then we will demonstrate how to use RAIL degraders to apply effects to those samples.\n",
    "\n",
    "### Training and Estimation\n",
    "\n",
    "The RAIL Trainer modules \"train\" or \"inform\" models used to estimate p(z) given band fluxes (and potentially other information).\n",
    "\n",
    "The RAIL Estimation modules then use those same models to actually apply the model and extract the p(z) estimates.\n",
    "\n",
    "### p(z) Validation \n",
    "\n",
    "The RAIL Validator module applies various metrics \n",
    "\n",
    "### p(z) to n(z) Summarization\n",
    "\n",
    "The RAIL Summarization modules convert per-galaxy p(z) posteriors to ensemble n(z) estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c7d44f",
   "metadata": {},
   "source": [
    "###  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295da0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerquisites, os, and numpy\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac482bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various rail modules\n",
    "import rail\n",
    "from rail.creation.degradation import LSSTErrorModel, InvRedshiftIncompleteness, LineConfusion, QuantityCut\n",
    "from rail.creation.engines.flowEngine import FlowEngine, FlowPosterior\n",
    "from rail.core.data import TableHandle\n",
    "from rail.core.stage import RailStage\n",
    "from rail.core.utilStages import ColumnMapper, TableConverter\n",
    "\n",
    "from rail.estimation.algos.bpz_lite import BPZ_lite\n",
    "from rail.estimation.algos.trainZ import Train_trainZ, TrainZ\n",
    "from rail.estimation.algos.sklearn_nn import Train_SimpleNN, SimpleNN\n",
    "from rail.estimation.algos.randomPZ import RandomPZ\n",
    "from rail.estimation.algos.flexzboost import Train_FZBoost, FZBoost\n",
    "\n",
    "from rail.evaluation.evaluator import Evaluator\n",
    "\n",
    "from rail.summarization.algos.naiveStack import NaiveStack\n",
    "from rail.summarization.algos.pointEstimateHist import PointEstimateHist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-gregory",
   "metadata": {},
   "source": [
    "RAIL now uses ceci as a back-end, which takes care of a lot of file I/O decisions to be consistent with other choices in DESC.\n",
    "\n",
    "This bit effectively overrides a ceci default to prevent overwriting previous results, generally good but not necessary for this demo.\n",
    "\n",
    "The `DataStore` uses `DataHandle` objects to keep track of the connections between the various stages.  When one stage returns a `DataHandle` and then you pass that `DataHandle` to another stage, the underlying code can establish the connections needed to build a reproducilble pipeline.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00fc25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-polymer",
   "metadata": {},
   "source": [
    "The path stuff for setup establishes where to find a pre-trained creator's file.\n",
    "TODO: make an issue for trainign one through RAIL rather than externally with pzflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAIL_DIR = os.path.join(os.path.dirname(rail.__file__), '..')\n",
    "flow_file = os.path.join(RAIL_DIR, 'examples/goldenspike/data/pretrained_flow.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-premises",
   "metadata": {},
   "source": [
    "Here we need a few configuration parameters to deal with differences in data schema between existing PZ codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6758a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ['u','g','r','i','z','y']\n",
    "band_dict = {band:f'mag_{band}_lsst' for band in bands}\n",
    "rename_dict = {f'mag_{band}_lsst_err':f'mag_err_{band}_lsst' for band in bands}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-wyoming",
   "metadata": {},
   "source": [
    "## Make mock data\n",
    "\n",
    "First, we make the stages.\n",
    "Note that training and test data will refer to the same flow file for the Engine, but they otherwise have different stages.\n",
    "\n",
    "### Training sample\n",
    "\n",
    "For the training sample we will:\n",
    "\n",
    "1. Use the Flow to produce some synthetic data\n",
    "2. Use the LSSTErrorModel to smear the data\n",
    "3. Use the ColumnMapper to rename some of the columns, as needed by FlowPosterior module\n",
    "4. Use the FlowPosterior to estimate the redshift posteriors for the degraded sample\n",
    "5. Use the TableConverter to convert the data to a numpy dictionary, which will be stored in a hdf5 file with the same schema as the DC2 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_engine_train = FlowEngine.make_stage(name='flow_engine_train', \n",
    "                                          flow=flow_file, n_samples=50)\n",
    "      \n",
    "lsst_error_model_train = LSSTErrorModel.make_stage(name='lsst_error_model_train',\n",
    "                                                   bandNames=band_dict)\n",
    "                \n",
    "col_remapper_train = ColumnMapper.make_stage(name='col_remapper_train', hdf5_groupname='',\n",
    "                                             columns=rename_dict)\n",
    "\n",
    "flow_post_train = FlowPosterior.make_stage(name='flow_post_train',\n",
    "                                           column='redshift', flow=flow_file,\n",
    "                                           grid=np.linspace(0., 5., 21))\n",
    "\n",
    "table_conv_train = TableConverter.make_stage(name='table_conv_train', output_format='numpyDict', \n",
    "                                             seed=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_orig = flow_engine_train.sample(50, 12345)\n",
    "train_data_errs = lsst_error_model_train(train_data_orig)\n",
    "train_data_pq = col_remapper_train(train_data_errs)\n",
    "train_data_post = flow_post_train.get_posterior(train_data_pq, 'redshift', err_samples=None)\n",
    "train_data = table_conv_train(train_data_pq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-decline",
   "metadata": {},
   "source": [
    "TODO: print some data, like column names\n",
    "\n",
    "### Testing sample\n",
    "\n",
    "For the testing data we are going to apply a couple of extra degradation effects to the data.  This will allow us to see how the trained models perform with imperfect training data.\n",
    "\n",
    "More details about the degraders are available in the `rail/examples/creation/degradation_demo.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655434f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_engine_test = FlowEngine.make_stage(name='flow_engine_test', \n",
    "                                         flow=flow_file, n_samples=50,\n",
    "                                         seed=12345)\n",
    "\n",
    "lsst_error_model_test = LSSTErrorModel.make_stage(name='lsst_error_model_test',\n",
    "                                                  bandNames=band_dict)\n",
    "\n",
    "inv_redshift = InvRedshiftIncompleteness.make_stage(name='inv_redshift',\n",
    "                                                    pivot_redshift=1.0)\n",
    "\n",
    "line_confusion = LineConfusion.make_stage(name='line_confusion', \n",
    "                                          true_wavelen=5007., wrong_wavelen=3727., frac_wrong=0.05)\n",
    "\n",
    "quantity_cut = QuantityCut.make_stage(name='quantity_cut',    \n",
    "                                      cuts={'mag_i_lsst': 25.3})\n",
    "\n",
    "col_remapper_test = ColumnMapper.make_stage(name='col_remapper_test', columns=rename_dict)\n",
    "   \n",
    "table_conv_test = TableConverter.make_stage(name='table_conv_test', output_format='numpyDict')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_orig = flow_engine_test.sample(50, 12345)\n",
    "test_data_errs = lsst_error_model_test(test_data_orig)\n",
    "test_data_inc = inv_redshift(test_data_errs)\n",
    "test_data_conf = line_confusion(test_data_inc)\n",
    "test_data_cut = quantity_cut(test_data_conf)\n",
    "test_data_pq = col_remapper_test(test_data_cut)\n",
    "test_data = table_conv_test(test_data_pq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-surgeon",
   "metadata": {},
   "source": [
    "TODO: print some data, e.g. column names showing difference from test data\n",
    "\n",
    "## \"Inform\" some estimators\n",
    "\n",
    "More details about the process of \"informing\" or \"training\" the models used by the estimators is available in the `rail/examples/estimation/RAIL_estimation_demo.ipynb` notebook.\n",
    "\n",
    "{inform refers to any prior info, usually training set but also template library, relevant to hybrid estimators, consistency in how they're called}\n",
    "\n",
    "TODO: change `train` to `inform` in corresponding RAIL modules. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48eed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_trainZ = Train_trainZ.make_stage(name='inform_trainZ', input='inprogress_output_table_conv_train.hdf5', \n",
    "                                        model='trainZ.pkl', hdf5_groupname='')\n",
    "\n",
    "# train_simpleNN = Train_SimpleNN.make_stage(name='train_simpleNN', input='inprogress_output_table_conv_train.hdf5', \n",
    "#                                            model_file='simpleNN.pkl', hdf5_groupname='')\n",
    "\n",
    "# train_fzboost = Train_FZBoost.make_stage(name='train_FZBoost', input='inprogress_output_table_conv_train.pq', \n",
    "#                                          model_file='fzboost.pkl', hdf5_groupname='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b544823",
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_trainZ.inform(train_data)\n",
    "#train_simpleNN.inform(train_data)\n",
    "#train_fzboost.inform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-utilization",
   "metadata": {},
   "source": [
    "## Estimate photo-z posteriors\n",
    "\n",
    "More details about the estimators is available in the `rail/examples/estimation/RAIL_estimation_demo.ipynb` notebook.\n",
    "\n",
    "`randomPZ` is a very simple class that does not actually predict a meaningful photo-z, instead it produces a randomly drawn Gaussian for each galaxy.<br>\n",
    "`trainZ` is our \"pathological\" estimator, it makes a PDF from a histogram of the training data and assigns that PDF to every galaxy.<br>\n",
    "`BPZ_lite` is a template-based code that outputs the posterior estimated given a specific template set and Bayesian prior. See Benitez (2000) for more details.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68120a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_bpz = BPZ_lite.make_stage(name='estimate_bpz', hdf5_groupname='', columns_file='../estimation/configs/test_bpz.columns')\n",
    "\n",
    "estimate_trainZ = TrainZ.make_stage(name='estimate_trainZ', hdf5_groupname='', model=inform_trainZ.get_handle('model'))\n",
    "\n",
    "estimate_randomPZ = RandomPZ.make_stage(name='estimate_randomZ', hdf5_groupname='')\n",
    "\n",
    "#test_simpleNN = SimpleNN.make_stage(name='test_simpleNN', \n",
    "#                                    model_file='simpleNN.pkl')\n",
    "\n",
    "#test_fzboost = FZBoost.create(name='test_FZBoost', \n",
    "#                              model_file='fzboost.pkl', \n",
    "#                              aliases=dict(input='test_data', output='fzboost_estim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f490f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpz_estimated = estimate_bpz.estimate(test_data)\n",
    "trainZ_estimated = estimate_trainZ.estimate(test_data)\n",
    "randomPZ_estimated = estimate_randomPZ.estimate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-adelaide",
   "metadata": {},
   "source": [
    "## Evaluate the estimates\n",
    "\n",
    "Now we evaluate metrics on the estimates, separately for each estimator.  \n",
    "\n",
    "Each call to the `Evaluator.evaluate` will create a table with the various performance metrics. \n",
    "We will store all of these tables in a dictionary, keyed by the name of the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ac2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = dict(bpz=bpz_estimated, trainZ=trainZ_estimated)\n",
    "truth = test_data_orig\n",
    "\n",
    "result_dict = {}\n",
    "for key, val in eval_dict.items():\n",
    "    the_eval = Evaluator.make_stage(name=f'{key}_eval', truth=truth)\n",
    "    result_dict[key] = the_eval.evaluate(val, truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-wells",
   "metadata": {},
   "source": [
    "The Pandas DataFrame output format conveniently makes human-readable printouts of the metrics.  \n",
    "This next cell will convert everything to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320cc596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables_io\n",
    "results_tables = {key:tables_io.convertObj(val.data, tables_io.types.PD_DATAFRAME) for key,val in result_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tables['bpz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tables['trainZ']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-director",
   "metadata": {},
   "source": [
    "## Summarize the per-galaxy redshift constraints to make population-level distributions\n",
    "\n",
    "{introduce the summarizers}\n",
    "\n",
    "First we make the stages, then execute them, then plot the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c45122",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_estimate_test = PointEstimateHist.make_stage(name='point_estimate_test')\n",
    "naive_stack_test = NaiveStack.make_stage(name='naive_stack_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_estimate_ens = point_estimate_test.summarize(eval_dict['bpz'])\n",
    "naive_stack_ens = naive_stack_test.summarize(eval_dict['bpz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = naive_stack_ens.data.plot_native(xlim=(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b049a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = point_estimate_ens.data.plot_native(xlim=(0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d51ec",
   "metadata": {},
   "source": [
    "### Convert this to a `ceci` Pipeline\n",
    "\n",
    "Now that we have all these stages defined and configured, and that we have established the connections between them by passing `DataHandle` objects between them, we can build a `ceci` Pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb78a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ceci\n",
    "pipe = ceci.Pipeline.interactive()\n",
    "stages = [flow_engine_test, lsst_error_model_test, col_remapper_test, table_conv_test,\n",
    "          flow_engine_train, lsst_error_model_train, col_remapper_train, table_conv_train, \n",
    "          inv_redshift, line_confusion, quantity_cut,\n",
    "          inform_trainZ, estimate_bpz, estimate_trainZ, estimate_randomPZ,\n",
    "          point_estimate_test, naive_stack_test]\n",
    "for stage in stages:\n",
    "    pipe.add_stage(stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194bc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.initialize(dict(flow=flow_file), dict(output_dir='.', log_dir='.', resume=False), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a842a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.save('tmp_goldenspike.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0500636",
   "metadata": {},
   "source": [
    "### Read back the pipeline and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = ceci.Pipeline.read('tmp_goldenspike.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728303e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5238bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
