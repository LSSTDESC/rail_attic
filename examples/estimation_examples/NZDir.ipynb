{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2859224e-4ef2-44c8-940c-b911c5568f89",
   "metadata": {},
   "source": [
    "# The NZDir estimator\n",
    "\n",
    "Author: Sam Schmidt<br>\n",
    "Last successfully run: April 26, 2023<br>\n",
    "\n",
    "This is a quick demo of the NZDir estimator, it has been ported to RAIL based on Joe Zuntz's implementation in TXPipe here: https://github.com/LSSTDESC/TXPipe/blob/nz-dir/txpipe/nz_calibration.py<br>\n",
    "First off, let's load the relevant packages from RAIL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f216d3f-d09a-4788-8e8d-7b48e780f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rail\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables_io\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac92a6d-24d9-4413-8914-f99fe4f6d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.estimation.algos.NZDir import NZDir, Inform_NZDir\n",
    "from rail.core.data import TableHandle\n",
    "from rail.core.stage import RailStage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc589b4b-abb4-4e3b-9665-cc395f028bef",
   "metadata": {},
   "source": [
    "Next, let's set up the Data Store, so that our RAIL module will know where to fetch data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd545b-430e-4e93-b761-3c733ccd13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026158f-14a3-4feb-aa0a-4bc5b31108ef",
   "metadata": {},
   "source": [
    "Next, we'll load some data into the Data Store:<br>\n",
    "`test_dc2_training_9816.hdf5` contains ~10,000 galaxies from healpix 9816 of the cosmoDC2 \"truth\" catalog, and the \"validation\" data set contains ~20,000 galaxies from this same healpix pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d40906-f918-4a97-bf33-c6cb4c5e1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.core.utils import RAILDIR\n",
    "trainFile = os.path.join(RAILDIR, 'rail/examples_data/testdata/test_dc2_training_9816.hdf5')\n",
    "testFile = os.path.join(RAILDIR, 'rail/examples_data/testdata/test_dc2_validation_9816.hdf5')\n",
    "training_data = DS.read_file(\"training_data\", TableHandle, trainFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc63b9-bfc5-4e8d-b415-dc13c0ec0c18",
   "metadata": {},
   "source": [
    "Let's read test data in with tables_io, and then split it up into several tomographic bins. We can mock up some simple \"tomographic\" bins via their true redshift.  The degrader expects a pandas DataFrame, so we will create three dataframes for each of a low, mid, and hi redshift sample.  Let's also add a weight column to the test data while we are at it, this will be used later by the NZDir algorithm (for now we'll set all weights to one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56117ad0-fbd9-4f29-b2a9-80bdf431cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = tables_io.read(testFile)['photometry']\n",
    "df = tables_io.convert(rawdata, tType=tables_io.types.PD_DATAFRAME)\n",
    "szcol = rawdata['redshift']\n",
    "numintest = len(szcol)\n",
    "df['weight'] = np.ones(numintest, dtype='float')\n",
    "lowmask = (szcol<=0.75)\n",
    "midmask = np.logical_and(szcol>.75, szcol<1.25)\n",
    "himask = (szcol>=1.25)\n",
    "lowzdata = df[lowmask]\n",
    "midzdata = df[midmask]\n",
    "hizdata = df[himask]\n",
    "low_bin = DS.add_data(\"lowz_bin\", lowzdata, TableHandle)\n",
    "mid_bin = DS.add_data(\"midz_bin\", midzdata, TableHandle)\n",
    "hi_bin = DS.add_data(\"hiz_bin\", hizdata, TableHandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57dffba-0d28-4270-8604-37df3e42fb8d",
   "metadata": {},
   "source": [
    "# The algorithm:\n",
    "The NZDir estimator tries to reconstruct the redshift distribution for an unknown sample (which we'll alternately call the \"photometric sample\", as it has photometric, but not spectroscopic information for each galaxy) by finding spectroscopic galaxies with similar magnitudes/colors and assigning a redshift based on those similarly-colored objects.  In practice, this particular algorithm actually reverses that process, it defines a neighborhood around each spectroscopic object (based on the distance to the Nth nearest neighbor, where N is defined by the user via the parameter `n_neigh`).  Then, it loops over the set of all spectroscopic objects and adds its (weighted) redshift to a histogram for each photometric object that it finds within the annulus.  This process is more efficient computationally, and has the benefit of automatically \"ignoring\" photometric objects that have no similarly colored spectroscopic objects nearby.  *However*, that could also be seen as a limitation, as if there are areas of color^N space not covered by your training sample, those galaxies will be \"skipped\" when assembling the tomographic redshift N(z) estimate, which can lead to biased results, as we will show later in this demo.  <br>\n",
    "\n",
    "Like PDF estimators, the algorithm is broken up into an \"inform\" stage and an \"estimate\" stage.  The inform stage creates the neighbors for the spectroscopic samples and calculates the distance to the Nth nearest neighbor that is used to determine annulus checks around each spec-z object.  These quantites are stored in a specified model file that is loaded and used by the estimate stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6b083-6acf-4bd3-bb2a-3d6ac4e28720",
   "metadata": {},
   "source": [
    "Let's also add a \"weight column\" to the training data to test functionality.  For simplicity we already set the weights to 1.0 for all photometric galaxies a few cells above, and now let's set weights of 0.5 for all spectroscopic galaxies.  This should have no impact on the recovery compared to having no weights included.  Note that if weights are not included, the algorithm will set all weights to 1.0.  However, these weights could be used in more realistic analyses to reweight training or test samples to account for various biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb9585-194b-4637-a485-371c044940c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numinphot = len(training_data()['photometry']['redshift'])\n",
    "training_data()['photometry']['weight'] = np.ones(numinphot, dtype='float')*0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcad1c-edd0-4d49-a195-6e50e06ce8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "zmin = 0.0\n",
    "zmax = 3.0\n",
    "xmanybins = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d4835-2ff6-4206-b017-cdf1d7cad828",
   "metadata": {},
   "source": [
    "Now, let's set up or estimator, first creating a stage for the informer.  We define any input variables in a dictionary and then use that with `make_stage` to create an instance of our NZDir summarizer.  We'll create a histogram of 25 bins, using 5 nearest neighbors to define our specz neighborhood, and above we defined our bin column as \"bin\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be250d-9ff7-43e5-9d8a-e31279c1eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nzdir = Inform_NZDir.make_stage(name='train_nzdir', n_neigh=5,\n",
    "                                      szweightcol='weight', model=\"NZDir_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f5c76-b3fb-4ddc-a78e-afdd04a28e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nzdir.inform(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa43a5-da29-4706-97ca-0bea05fce2d2",
   "metadata": {},
   "source": [
    "Now, let's set up our NZDir estimator, providing parameters for the redshift grid, photomtetric weight column, and the model that we created with the informer.  Note that NZDir returns N bootstrap samples rather than just a single distribution.  The code draws bootstrap samples from the spectroscopic sample to use as input as the training data.  `nsamples` can be used to set the number of bootstrap samples returned, for this demo we will only generate 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4367d-f1db-4cf1-ae26-80500757149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summdict = dict(leafsize=20, zmin=zmin, zmax=zmax, nzbins=xmanybins, nsamples=20,\n",
    "                phot_weightcol='weight', model=\"NZDir_model.pkl\", hdf5_groupname='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287dca6-43ea-452b-add5-3eca2e9613cf",
   "metadata": {},
   "source": [
    "We have three tomographic bins, we can make a stage and run each one in a loop.  To run our Nz Estimator we just need to run `estimate` with arguments for the test and training data handles as named in the Data Store: <br>\n",
    "The code uses a fast Nearest Neighbor calculation and KDTree calculation, so this should run very fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b8e32-f218-46f7-a99e-661d86424107",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bin_ens = {}\n",
    "binnames = ['low', 'mid', 'hi']\n",
    "bin_datasets = [low_bin, mid_bin, hi_bin]\n",
    "for bin, indata in zip(binnames, bin_datasets):\n",
    "    nzsumm = NZDir.make_stage(name=f'nzsumm_{bin}', **summdict)\n",
    "    bin_ens[f'{bin}'] = nzsumm.estimate(indata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38e44b-1a7e-4368-b70f-76c9f2882c3b",
   "metadata": {},
   "source": [
    "indeed, for our 20,000 test and 10,000 training galaxies, it takes less than a second to run all three bins!  Now, let's plot our estimates and compare to the true distributions in our tomo bins.  While the ensembles actually contain 20 distributions, we will plot only the first bootstrap realization for each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab48914c-16f4-4aba-9cd5-e12654a5f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "samebins = np.linspace(zmin,zmax, xmanybins)\n",
    "binsize= samebins[1]-samebins[0]\n",
    "bincents = 0.5*(samebins[1:] + samebins[:-1])\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(20,6))\n",
    "bin_datasets = [low_bin, mid_bin, hi_bin]\n",
    "binnames = ['low', 'mid', 'hi']\n",
    "for ii, (bin, indata) in enumerate(zip(binnames, bin_datasets)):\n",
    "    truehist, bins = np.histogram(indata()['redshift'], bins=samebins)\n",
    "    norm = np.sum(truehist)*binsize\n",
    "    truehist = np.array(truehist)/norm\n",
    "    bin_ens[f'{bin}']().plot_native(axes=axs[ii],label=\"DIR estimate\")\n",
    "    axs[ii].bar(bincents, truehist,alpha=0.55, width=binsize, color='b', label=\"true redshift distn\")\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.title(\"representative training data\", fontsize=15)\n",
    "plt.xlabel(\"redshift\", fontsize=12)\n",
    "plt.ylabel(\"N(z)\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e803663-7e59-4ee0-9afc-2c50be21dee3",
   "metadata": {},
   "source": [
    "# Non-representative data\n",
    "\n",
    "That looks very nice, while there is a little bit of \"slosh\" outside of each bin, we have a relatively compact and accurate representation from the DIR method!  This makes sense, as our training and test data are drawn from the same underlying distribution (in this case cosmoDC2_v1.1.4).  However, how will things look if we are missing chunks of data, or have incorrect redshifts in our spec-z sample?  We can use RAIL's degradation modules to do just that: place incorrect redshifts for percentage of the training data, and we can make a magnitude cut that will limite the redshift and color range of our training data: <br>\n",
    "\n",
    "Let's import the necessary modules from rail.creation.degradation, we will put in \"line confusion\" for 5% of our sample, and then cut the sample at magnitude 23.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e32a2-622f-4d2a-b73e-480117c2cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.creation.degradation import LineConfusion, QuantityCut\n",
    "from rail.core.data import PqHandle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ca3e9-75e6-48bd-843f-8ad415a7188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_confusion = LineConfusion.make_stage(name='line_confusion', hdf5_groupname='photometry',\n",
    "                                          true_wavelen=5007., wrong_wavelen=3727., frac_wrong=0.05)\n",
    "\n",
    "quantity_cut = QuantityCut.make_stage(name='quantity_cut', hdf5_groupname='photometry',\n",
    "                                      cuts={'mag_i_lsst': 23.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f0a60f-eea8-431e-b214-0da1597c9b58",
   "metadata": {},
   "source": [
    "The degrader expects a pandas dataframe, so let's construct one and add it to the data store, we'll strip out the 'photometry' hdf5 while we're at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b25ba-7aba-429d-af47-9cf4d1b3894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrade_df = pd.DataFrame(training_data.data['photometry'])\n",
    "degrade_data = DS.add_data(\"degrade_data\", degrade_df, PqHandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8110744-6bf4-4a7d-a7f3-e950341d6706",
   "metadata": {},
   "source": [
    "Now, apply our degraders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ac0da-122f-4fbf-b4fa-2c1a3eb966e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_conf = line_confusion(degrade_data)\n",
    "train_data_cut = quantity_cut(train_data_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dabe01a-3bbd-490d-974d-f41e2a909186",
   "metadata": {},
   "source": [
    "Let's plot our trimmed training sample, we see that we have fewer galaxies, so we'll be subject to more \"shot noise\"/discretization of the redshifts, and we are very incomplete at high redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9e0b6-6cb9-430b-8988-af85e7ad284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare original specz data to degraded data\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "xbins = np.linspace(0,3,41)\n",
    "plt.hist(training_data()['photometry']['redshift'],bins=xbins,alpha=0.75, label='original training data');\n",
    "plt.hist(train_data_cut()['redshift'], bins=xbins,alpha=0.75, label='trimmed training data');\n",
    "plt.legend(loc='upper right', fontsize=15)\n",
    "plt.xlabel(\"redshift\", fontsize=15)\n",
    "plt.ylabel(\"N\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03acb4f-cc54-43bb-a2ce-ba0a9edface2",
   "metadata": {},
   "source": [
    "Let's re-run our estimator on the same test data but now with our incomplete training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe5a6b-dfd1-4df9-a4f9-f28e9d39d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xinformdict = dict(n_neigh=5, bincol=\"bin\", szweightcol='weight',\n",
    "                   model=\"NZDir_model_incompl.pkl\", hdf5_groupname='')\n",
    "newsumm_inform = Inform_NZDir.make_stage(name='newsumm_inform', **xinformdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00b7dc-af25-4dec-8ea6-8e96f0885e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsumm_inform.inform(train_data_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b01031c-c2b0-4935-886d-14ca4c7eaf22",
   "metadata": {},
   "source": [
    "Now we need to re-run our tomographic bin estimates with this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568b806-90d0-4ee9-a973-660f460aa29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xestimatedict = dict(leafsize=20, zmin=zmin, zmax=zmax, nzbins=xmanybins, hdf5_groupname='', nsamples=20,\n",
    "                     phot_weightcol='weight', model=newsumm_inform.get_handle('model'))\n",
    "new_ens = {}\n",
    "binnames = ['low', 'mid', 'hi']\n",
    "bin_datasets = [low_bin, mid_bin, hi_bin]\n",
    "for bin, indata in zip(binnames, bin_datasets):\n",
    "    nzsumm = NZDir.make_stage(name=f'nzsumm_{bin}', **xestimatedict)\n",
    "    new_ens[f'{bin}'] = nzsumm.estimate(indata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d255d-f634-47ae-9eca-02e3b134d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(20,6))\n",
    "samebins = np.linspace(0,3, xmanybins)\n",
    "binsize= samebins[1]-samebins[0]\n",
    "bincents = 0.5*(samebins[1:] + samebins[:-1])\n",
    "bin_datasets = [low_bin, mid_bin, hi_bin]\n",
    "binnames = ['low', 'mid', 'hi']\n",
    "\n",
    "for ii, (bin, indata) in enumerate(zip(binnames, bin_datasets)):\n",
    "    truehist, bins = np.histogram(indata.data['redshift'], bins=samebins)\n",
    "    norm = np.sum(truehist)*binsize\n",
    "    truehist = np.array(truehist)/norm\n",
    "    new_ens[f'{bin}']().plot_native(axes=axs[ii],label=\"DIR estimate\")\n",
    "    axs[ii].bar(bincents, truehist,alpha=0.55, width=binsize, color='b', label=\"true redshift distn\")\n",
    "axs[0].legend(loc='upper right', fontsize=12)\n",
    "axs[1].set_title(\"non-representative training data\", fontsize=15)\n",
    "axs[1].set_xlabel(\"redshift\", fontsize=15)\n",
    "axs[0].set_ylabel(\"N(z)\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c05b6af-4773-4326-bd9c-36050b9c42fe",
   "metadata": {},
   "source": [
    "We see that the high redshift bin, where our training set was very incomplete, looks particularly bad, as expected.  Bins 1 and 2 look surprisingly good, which is a promising sign that, even when a brighter magnitude cut is enforced, this method is sometimes still able to produce reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039246e8-f06f-4dd2-afcb-cbfac37ab07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4450d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
