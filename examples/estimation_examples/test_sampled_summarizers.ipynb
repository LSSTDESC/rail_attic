{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c697f8c4",
   "metadata": {},
   "source": [
    "Author: Sam Schmidt <br>\n",
    "Last successfully run: April 26, 2023<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042969c1-911f-4c35-a1dd-f90befe099dc",
   "metadata": {},
   "source": [
    "June 28 update:\n",
    "I modified the summarizers to output not just N sample N(z) distributions (saved to the file specified via the `output` keyword), but also the single fiducial N(z) estimate (saved to the file specified via the `single_NZ` keyword).  I also updated NZDir and included it in this example notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96186af9-95cb-446d-bfa6-93e3508a9d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rail\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables_io\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ac396-9707-4cd2-bcf5-56c526c91691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.estimation.algos.knnpz import Inform_KNearNeighPDF, KNearNeighPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c147b-5cef-4d78-932e-7ae7df6dbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.estimation.algos.varInference import VarInferenceStack\n",
    "from rail.estimation.algos.naiveStack import NaiveStack\n",
    "from rail.estimation.algos.pointEstimateHist import PointEstimateHist\n",
    "from rail.estimation.algos.NZDir import Inform_NZDir, NZDir\n",
    "from rail.core.data import TableHandle, QPHandle\n",
    "from rail.core.stage import RailStage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7174c-f618-411a-aaee-873dabfaa5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e6b52-76be-41a7-a5e4-9d9b9472b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddccd67d-7321-4e8b-b4c0-e5d7e6aacc52",
   "metadata": {},
   "source": [
    "To create some N(z) distributions, we'll want some PDFs to work with first, for a quick demo let's just run some photo-z's using the KNearNeighPDF estimator using the 10,000 training galaxies to generate ~20,000 PDFs using data from healpix 9816 of cosmoDC2_v1.1.4 that are included in the RAIL repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0f0f0-a38a-42ec-b4f6-696127439140",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_dict = dict(zmin=0.0, zmax=3.0, nzbins=301, trainfrac=0.75,\n",
    "                sigma_grid_min=0.01, sigma_grid_max=0.07, ngrid_sigma=10,\n",
    "                nneigh_min=3, nneigh_max=7, hdf5_groupname='photometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ef13c-0ab5-4b5e-ae67-d138854be4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pz_train = Inform_KNearNeighPDF.make_stage(name='inform_KNN', model='demo_knn.pkl', **knn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecdeb24-b5d9-4985-8968-a87158a53757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the example healpix 9816 data and stick in the DataStore\n",
    "from rail.core.utils import RAILDIR\n",
    "trainFile = os.path.join(RAILDIR, 'rail/examples_data/testdata/test_dc2_training_9816.hdf5')\n",
    "testFile = os.path.join(RAILDIR, 'rail/examples_data/testdata/test_dc2_validation_9816.hdf5')\n",
    "training_data = DS.read_file(\"training_data\", TableHandle, trainFile)\n",
    "test_data = DS.read_file(\"test_data\", TableHandle, testFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920e65c-776f-43f9-b96c-f88ac29ee460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train knnpz\n",
    "pz_train.inform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663b2c6-2163-4efb-ae32-413f0e40790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pz = KNearNeighPDF.make_stage(name='KNN', hdf5_groupname='photometry',\n",
    "                              model=pz_train.get_handle('model'))\n",
    "qp_data = pz.estimate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c63e8af-fd52-4ada-8d20-3e0e7f47b0f7",
   "metadata": {},
   "source": [
    "So, `qp_data` now contains the 20,000 PDFs from KNearNeighPDF, we can feed this in to three summarizers to generate an overall N(z) distribution.  We won't bother with any tomographic selections for this demo, just the overall N(z).  It is stored as `qp_data`, but has also been saved to disk as `output_KNN.fits` as an astropy table.  If you want to read in this data to grab the qp Ensemble at a later stage, you can do this via qp with a `ens = qp.read(\"output_KNN.fits\")`\n",
    "\n",
    "I coded up **quick and dirty** bootstrap versions of the `NaiveStack`, `PointEstimateHist`, and `VarInference` sumarizers.  These are not optimized, not parallel (issue created for future update), but they do produce N different bootstrap realizations of the overall N(z) which are returned as a qp Ensemble (Note: the previous versions of these degraders returned only the single overall N(z) rather than samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f351b58-29b0-4831-b5ac-6ab0969c4d3d",
   "metadata": {},
   "source": [
    "# Naive Stack\n",
    "\n",
    "Naive stack just \"stacks\" i.e. sums up, the PDFs and returns a qp.interp distribution with bins defined by np.linspace(zmin, zmax, nzbins), we will create a stack with 41 bins and generate 20 bootstrap realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc3867-d992-428f-abb5-8f019fb18ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacker = NaiveStack.make_stage(zmin=0.0, zmax=3.0, nzbins=41, nsamples=20, output=\"Naive_samples.hdf5\", single_NZ=\"NaiveStack_NZ.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b373d-54be-4ea3-9e1c-1f6384d2caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_results = stacker.summarize(qp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241b499f-b9f3-4780-b909-4383d1160682",
   "metadata": {},
   "source": [
    "The results are now in naive_results, but because of the DataStore, the actual *ensemble* is stored in `.data`, let's grab the ensemble and plot a few of the bootstrap sample N(z) estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94b5f1-7604-4735-a157-0dfd4388eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "newens = naive_results.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59607d-6ded-4f95-bb31-13d03176fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(8,6))\n",
    "for i in range(0, 20, 2):\n",
    "    newens[i].plot_native(axes=axs, label=f\"sample {i}\")\n",
    "axs.plot([0,3],[0,0],'k--')\n",
    "axs.set_xlim(0,3)\n",
    "axs.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3cb989-7fb2-4e9a-b5e8-ae7b430d0720",
   "metadata": {},
   "source": [
    "The summarizer also outputs a **second** file containing the fiducial N(z).  We saved the fiducial N(z) in the file \"NaiveStack_NZ.hdf5\", let's grab the N(z) estimate with qp and plot it with the native plotter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e53742-01ac-46c6-a2cd-bcccec1c66b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_nz = qp.read(\"NaiveStack_NZ.hdf5\")\n",
    "naive_nz.plot_native(xlim=(0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e66b7-28f0-483d-a627-00a88fec063c",
   "metadata": {},
   "source": [
    "# Point Estimate Hist\n",
    "PointEstimateHist takes the point estimate mode of each PDF and then histograms these, we'll again generate 41 bootstrap samples of this and plot a few of the resultant histograms.\n",
    "Note: For some reason the plotting on the histogram distribution in qp is a little wonky, it appears alpha is broken, so this plot is not the best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b1cd9-61d8-42ce-882b-db8ab17f4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointy = PointEstimateHist.make_stage(zmin=0.0, zmax=3.0, nzbins=41, nsamples=20, single_NZ=\"point_NZ.hdf5\", output=\"point_samples.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164752a4-e015-40b3-9261-396c58b005aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pointy_results = pointy.summarize(qp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a12cb5-9b20-4d19-8807-e76f692b81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pens = pointy_results.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f8319-90b4-4cf3-9692-3f043cdfb21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(8,6))\n",
    "pens[0].plot_native(axes=axs, fc = [0, 0, 1, 0.01])\n",
    "pens[1].plot_native(axes=axs, fc = [0, 1, 0, 0.01])\n",
    "pens[4].plot_native(axes=axs, fc = [1, 0, 0, 0.01])\n",
    "axs.set_xlim(0,3)\n",
    "axs.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f669af79-03ff-4e97-bd0b-cbb2c4a2e8fb",
   "metadata": {},
   "source": [
    "Again, we have saved the fiducial N(z) in a separate file, \"point_NZ.hdf5\", we could read that data in if we desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390c5cb-9899-4e56-a872-04d1927a0dc7",
   "metadata": {},
   "source": [
    "# varInference\n",
    "\n",
    "VarInference implements Markus' variational inference scheme and returns qp.interp gridded distribution. varInference tends to get a little wonky if you use too many bins, so we'll only use 25 bins. Again let's generate 20 samples and plot a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d50d0-fc3f-4adb-a63c-f45dba5bb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner=VarInferenceStack.make_stage(name='test_varinf', zmin=0.0,zmax=3.0,nzbins=25, niter=10, nsamples=20,\n",
    "                                    output=\"sampletest.hdf5\", single_NZ=\"varinf_NZ.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e2d0d-b615-4984-b01a-0dc946e3aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varinf_results = runner.summarize(qp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef0943-278e-4573-98de-e902930451df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vens = varinf_results.data\n",
    "vens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2395767-c65c-4e5a-af0e-b243e6df4bab",
   "metadata": {},
   "source": [
    "Let's plot the fiducial N(z) for this distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597fba3b-d1b5-44eb-9631-4914e15768da",
   "metadata": {},
   "outputs": [],
   "source": [
    "varinf_nz = qp.read(\"varinf_NZ.hdf5\")\n",
    "varinf_nz.plot_native(xlim=(0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42416fa7-0cce-4fed-965f-616eb916a78e",
   "metadata": {},
   "source": [
    "# NZDir\n",
    "NZDir is a different type of summarizer, taking a weighted set of neighbors to a set of training spectroscopic objects to reconstruct the redshift distribution of the photometric sample.  I implemented a bootstrap of the **spectroscopic data** rather than the photometric data, both because it was much easier computationally, and I think that the spectroscopic variance is more important to take account of than simple bootstrap of the large photometric sample.\n",
    "We must first run the `inform_NZDir` stage to train up the K nearest neigh tree used by NZDir, then we will run `NZDir` to actually construct the N(z) estimate.  \n",
    "\n",
    "Like PointEstimateHist NZDir returns a qp.hist ensemble of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b287c-07ba-48a4-9be9-d4e2ad5212ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_nz = Inform_NZDir.make_stage(n_neigh=8, hdf5_groupname=\"photometry\", model=\"nzdir_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23200cf-041c-4ab2-af06-d0b942442cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_nz.inform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd515f9f-4efa-416a-b6d0-96d088c9150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nzd = NZDir.make_stage(leafsize=20, zmin=0.0, zmax=3.0, nzbins=31, model=\"NZDir_model.pkl\", hdf5_groupname='photometry',\n",
    "                       output='NZDir_samples.hdf5', single_NZ='NZDir_NZ.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149c641-7644-4b72-a266-f674b9488a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "nzd_res = nzd.estimate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c59415-a461-4e3a-882b-b89d251cd3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nzd_ens = nzd_res.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179fdae-1532-4af4-a82d-853fb1849512",
   "metadata": {},
   "outputs": [],
   "source": [
    "nzdir_nz = qp.read(\"NZDir_NZ.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2256cba-90ce-425c-b891-8e09c8cf871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10,8))\n",
    "nzd_ens[0].plot_native(axes=axs, fc = [0, 0, 1, 0.01])\n",
    "nzd_ens[1].plot_native(axes=axs, fc = [0, 1, 0, 0.01])\n",
    "nzd_ens[4].plot_native(axes=axs, fc = [1, 0, 0, 0.01])\n",
    "axs.set_xlim(0,3)\n",
    "axs.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a870aa08-7de2-49c1-bd9a-cfda3eb17631",
   "metadata": {},
   "source": [
    "As we also wrote out the single estimate of N(z) we can read that data from the second file written (specified by the `single_NZ` argument given in NZDir.make_stage above, in this case \"NZDir_NZ.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a8570-ba5d-4a2e-828a-3d640e9133b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nzdir_nz = qp.read(\"NZDir_NZ.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af83178-683c-4c79-912a-65dafe1fc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "nzdir_nz.plot_native(xlim=(0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa7ee13-1f2e-4549-8bca-a5e3e78b61cd",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "All three results files are qp distributions, NaiveStack and varInference return qp.interp distributions while pointEstimateHist returns a qp.histogram distribution.  Even with the different distributions you can use qp functionality to do things like determine the means, modes, etc... of the distributions.  You could then use the std dev of any of these to estimate a 1 sigma \"shift\", etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55a38f-fc53-48b5-b829-4a1db5020f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "zgrid = np.linspace(0,3,41)\n",
    "names = ['naive', 'point', 'varinf', 'nzdir']\n",
    "enslist = [newens, pens, vens, nzd_ens]\n",
    "results_dict = {}\n",
    "for nm, en in zip(names, enslist):\n",
    "    results_dict[f'{nm}_modes'] = en.mode(grid=zgrid).flatten()\n",
    "    results_dict[f'{nm}_means'] = en.mean().flatten()\n",
    "    results_dict[f'{nm}_std'] = en.std().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36bded3-3d59-4e5e-9b24-bed8071b6218",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf54a8-e742-47c4-8953-e4a43874d740",
   "metadata": {},
   "source": [
    "You can also use qp to compute quantities the pdf, cdf, ppf, etc... on any grid that you want, much of the functionality of scipy.stats distributions have been inherited by qp ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e47e90-aaf5-4845-a447-aea88873a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "newgrid = np.linspace(0.005,2.995, 35)\n",
    "naive_pdf = newens.pdf(newgrid)\n",
    "point_cdf = pens.cdf(newgrid)\n",
    "var_ppf = vens.ppf(newgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544624cd-2423-4116-bb30-7271d07af5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(newgrid, naive_pdf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf49c73-ad72-41b2-b857-bc89c1a72e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(newgrid, point_cdf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16ddd0-a003-4a2e-accc-40343ad67d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(newgrid, var_ppf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9020a5f-2820-4b61-bf08-b29d9f14014d",
   "metadata": {},
   "source": [
    "# Shifts\n",
    "\n",
    "If you want to \"shift\" a PDF, you can just evaluate the PDF on a shifted grid, for example to shift the PDF by +0.0375 in redshift you could evaluate on a shifted grid.  For now we can just do this \"by hand\", we could easily implement `shift` functionality in qp, I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce39f4f-3100-439b-80db-cfd3c1dd362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_grid = np.linspace(0., 3., 41)\n",
    "shift_grid = def_grid - 0.0675\n",
    "native_nz = newens.pdf(def_grid)\n",
    "shift_nz = newens.pdf(shift_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b779f0-e6ca-457c-a7e3-51693daae476",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12,10))\n",
    "plt.plot(def_grid, native_nz[0], label=\"original\")\n",
    "plt.plot(def_grid, shift_nz[0], label=\"shifted +0.0675\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607dff41-495d-4da3-9c9e-56d2d60ba90f",
   "metadata": {},
   "source": [
    "You can estimate how much shift you might expect based on the statistics of our bootstrap samples, say the std dev of the means for the NZDir-derived distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9dabcd-0317-40a7-a2f2-eb7fc3eb9386",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['nzdir_means']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9018d58-7253-4b94-ae0e-62cf8ca6e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spread = np.std(results_dict['nzdir_means'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3994c8c8-9bc7-4016-8946-bf720afd9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "spread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b6442-68ce-4a7f-b7be-8727a364deec",
   "metadata": {},
   "source": [
    "Again, not a huge spread in predicted mean redshifts based solely on bootstraps, even with only ~20,000 galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4702828-564b-4286-8f33-bc712c0fb4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
