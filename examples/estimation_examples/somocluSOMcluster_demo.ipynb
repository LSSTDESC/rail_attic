{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211657f5",
   "metadata": {},
   "source": [
    "# SomocluSOMSummarizer demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd4514",
   "metadata": {},
   "source": [
    "Author: Ziang Yan, Sam Schmidt <br>\n",
    "Last successfully run: April 26, 2023<br>\n",
    "\n",
    "This notebook shows a quick demonstration of the use of the `SOMocluSummarizer` summarization module.  Algorithmically, this module is not very different from the NZDir estimator/summarizer.  NZDir operates by finding neighboring photometric points around spectroscopic objects.  SOMocluSummarizer takes a large training set of data in the `Inform_SOMocluUmmarizer` stage and trains a self-organized map (SOM) (using code from the `somoclu` package available at: https://github.com/peterwittek/somoclu/).  Once the SOM is set up, the \"best match unit\" are determined for both the photometric/unknown data and a set of spectroscopic data with known redshifts.  For each SOM cell, the algorithm constructs a histogram using the spectroscopic members mapped to that cell, and weights these by the number of photometric galaxies in that cell.  Both the photometric and spectroscopic datasets can also employ an optional weight per-galaxy. <br>\n",
    "\n",
    "The summarizer also identifies SOM cells that contain photometric data but do not contain and galaxies with a measured spec-z, and thus do not have an obvious redshift estimate.  It writes out the (raveled) SOM cell indices that contain \"uncovered\"/uncalibratable data to the file specified by the `uncovered_cluster_file` option as a list of integers.  The cellIDs and galaxy/objIDs for all photometric galaxies will be written out to the file specified by the `cellid_output` parameter.  Any galaxies in these cells should really be removed, and thus some iteration may be necessary in defining bin membership by looking at the properties of objects in these uncovered cells before a final N(z) is estimated, as otherwise a bias may be present.<br>\n",
    "\n",
    "The shape and number of cells used in constructing the SOM affects performance, as do several tuning parameters.  This paper, http://www.giscience2010.org/pdfs/paper_230.pdf gives a rough guideline that the number of cells should be of the order ~ 5 x sqrt (number of data rows x number of column rows), though this is a rough guide.  Some studies have found a 2D SOM that is more elongated in one direction to be preferential, while others claim that a square layout is optimal, the user can set the number of cells in each SOM dimension via the `n_rows` and `n_cols` parameters.  For more discussion on SOMs see the Appendices of this KiDS paper:  http://arxiv.org/abs/1909.09632.\n",
    "\n",
    "As with the other RAIL summarizers, we bootstrap the spectroscopic sample and return N bootstraps in an ensemble, along with a single fiducial N(z) estimate.<br>\n",
    "\n",
    "More specific details of the algorithm's set up will be described in the course of this notebook, along with some illustrative plots.\n",
    "\n",
    "Let's set up our dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e34d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pickle\n",
    "import rail\n",
    "import os\n",
    "import qp\n",
    "import tables_io\n",
    "from rail.core.data import TableHandle\n",
    "from rail.core.stage import RailStage\n",
    "from rail.estimation.algos.somocluSOM import Inform_somocluSOMSummarizer, somocluSOMSummarizer\n",
    "from rail.estimation.algos.somocluSOM import get_bmus, plot_som"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31350a6b",
   "metadata": {},
   "source": [
    "Next, let's set up the Data Store, so that our RAIL module will know where to fetch data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0306311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b20e562",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, let's grab some data files.  For the SOM, we will want to train on a fairly large, representative set that encompasses all of our expected data.  We'll grab a larger data file than we typically use in our demos to ensure that we construct a meaningful SOM.\n",
    "\n",
    "This data consists of ~150,000 galaxies from a single healpix pixel of the comsoDC2 truth catalog with mock 10-year magnitude errors added.  It is cut at a relatively bright i<23.5 magnitudes in order to concentrate on galaxies with particularly high S/N rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = \"./healpix_10326_bright_data.hdf5\"\n",
    "\n",
    "if not os.path.exists(training_file):\n",
    "  os.system('curl -O https://portal.nersc.gov/cfs/lsst/schmidt9/healpix_10326_bright_data.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# way to get big data file\n",
    "training_data = DS.read_file(\"training_data\", TableHandle, training_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b22337",
   "metadata": {},
   "source": [
    "Now, let's set up the inform stage for our summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8c8782",
   "metadata": {},
   "source": [
    "We need to define all of our necessary initialization params, which includes the following:<br>\n",
    "`name` (str): the name of our estimator, as utilized by ceci<br>\n",
    "`model` (str): the name for the model file containing the SOM and associated parameters that will be written by this stage<br>\n",
    "`hdf5_groupname` (str): name of the hdf5 group (if any) where the photometric data resides in the training file<br>\n",
    "`n_rows` (int): the number of dimensions in the y-direction for our 2D SOM<br>\n",
    "`m_columns` (int): the number of dimensions in the x-direction for our 2D SOM<br>\n",
    "`som_iterations` (int): the number of iteration steps during SOM training.  SOMs can take a while to converge, so we will use a fairly large number of 500,000 iterations.<br>\n",
    "`std_coeff` (float): the \"radius\" of how far to spread changes in the SOM <br>\n",
    "`som_learning_rate` (float): a number between 0 and 1 that controls how quickly the weighting function decreases.  SOM's are not guaranteed to converge mathematically, and so this parameter tunes how the response drops per iteration.  A typical values we might use might be between 0.5 and 0.75.<br>\n",
    "`column_usage` (str):  this value determines what values will be used to construct the SOM, valid choices are `colors`, `magandcolors`, and `columns`.  If set to `colors`, the code will take adjacent columns as specified in `usecols` to construct colors and use those as SOM inputs.  If set to `magandcolors` it will use the single column specfied by `ref_column_name` and the aforementioned colors to construct the SOM.  If set to `columns` then it will simply take each of the columns in `usecols` with no modification.  So, if a user wants to use K magnitudes and L colors, they can precompute the colors and specify all names in `usecols`.  NOTE: accompanying `usecols` you must have a `nondetect_val` dictionary that lists the replacement values for any non-detection-valued entries for each column, see the code for an example dictionary.  WE will set `column_usage` to colors and use only colors in this example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f3243",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_type = 'hexagonal'\n",
    "inform_dict = dict(model='output_SOMoclu_model.pkl', hdf5_groupname='photometry',\n",
    "                   n_rows=71, n_columns=71, \n",
    "                   gridtype = grid_type,\n",
    "                   std_coeff=12.0, som_learning_rate=0.75,\n",
    "                   column_usage='colors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62103ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_som = Inform_somocluSOMSummarizer.make_stage(name='inform_som', **inform_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706c5483",
   "metadata": {},
   "source": [
    "Let's run our stage, which will write out a file called `output_SOM_model.pkl`\n",
    "\n",
    "**NOTE for those using M1 Macs:** you may get an error like `wrap_train not found` when running the inform stage in the cell just below here.  If so, this can be solved by reinstalling somoclu from conda rather than pip with the command:\n",
    "```\n",
    "conda install -c conda-forge somoclu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inform_som.inform(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6571c528",
   "metadata": {},
   "source": [
    "Running the stage took ~1 minute wall time on a desktop Mac and ~3.5 minutes on NERSC Jupyter lab.  Remember, however, that in many production cases we would likely load a pre-trained SOM specifically tuned to the given dataset, and this inform stage would not be run each time.<br>\n",
    "Let's read in the SOM model file, which contains our som model and several of the parameters used in constructing the SOM, and needed by our summarization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_SOMoclu_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f64c6",
   "metadata": {},
   "source": [
    "To visualize our SOM, let's calculate the cell occupation of our training sample, as well as the mean redshift of the galaxies in each cell.  The SOM took colors as inputs, so we will need to construct the colors for our training set galaxie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc3594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ['u','g','r','i','z','y']\n",
    "bandnames = [f\"mag_{band}_lsst\" for band in bands]\n",
    "ngal = len(training_data.data['photometry']['mag_i_lsst'])\n",
    "colors = np.zeros([5, ngal])\n",
    "for i in range(5):\n",
    "    colors[i] = training_data.data['photometry'][bandnames[i]] - training_data.data['photometry'][bandnames[i+1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a1512b",
   "metadata": {},
   "source": [
    "We can calculate the best SOM cell using the get_bmus() function defined in somocluSOM.py, which will return the 2D SOM coordinates for each galaxy. Then we group the SOM cells into 100 hierarchical clusters and calculate the occupation and mean redshift in each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c3f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOM = model['som']\n",
    "bmu_coordinates = get_bmus(SOM, colors.T, 1000).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as sc\n",
    "\n",
    "n_clusters = 100\n",
    "algorithm = sc.AgglomerativeClustering(n_clusters=n_clusters, linkage='complete')\n",
    "SOM.cluster(algorithm)\n",
    "som_cluster_inds = SOM.clusters.reshape(-1)\n",
    "phot_pixel_coords = np.ravel_multi_index(bmu_coordinates, (71, 71))\n",
    "\n",
    "phot_som_clusterind = som_cluster_inds[phot_pixel_coords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88c4e3",
   "metadata": {},
   "source": [
    "First, let's visualize our hierarchical clusters by plotting the SOM cells grouped into each cluster number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec1770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cellid = np.zeros_like(SOM.umatrix).reshape(-1)\n",
    "for i in range(n_clusters):\n",
    "    cellid[som_cluster_inds==i] = i\n",
    "cellid = cellid.reshape(SOM.umatrix.shape)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,12))\n",
    "plot_som(ax, cellid.T, grid_type=grid_type, colormap=cm.coolwarm, cbar_name='cell ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16f1b2",
   "metadata": {},
   "source": [
    "we see variations in number of cells in each grouping and geometry, but mostly nice contiguous cell chunks.  Next, let's plot the cell occupation and mean redshift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24936593",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanszs = np.zeros_like(SOM.umatrix).reshape(-1)\n",
    "cellocc = np.zeros_like(SOM.umatrix).reshape(-1)\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    meanszs[som_cluster_inds==i] = np.mean(training_data.data['photometry']['redshift'][phot_som_clusterind==i])\n",
    "    cellocc[som_cluster_inds==i] = (phot_som_clusterind==i).sum()\n",
    "meanszs = meanszs.reshape(SOM.umatrix.shape)\n",
    "cellocc = cellocc.reshape(SOM.umatrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c425963",
   "metadata": {},
   "source": [
    "Here is the cluster occupation distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6bbcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,12))\n",
    "plot_som(ax, cellocc.T, grid_type=grid_type, colormap=cm.coolwarm, cbar_name='cell occupation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cfdad",
   "metadata": {},
   "source": [
    "And here is the mean redshift per cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16445813",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,12))\n",
    "plot_som(ax, meanszs.T, grid_type=grid_type, colormap=cm.coolwarm, cbar_name='mean redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fa6ea",
   "metadata": {},
   "source": [
    "Now that we have illustrated what exactly we have constructed, let's use the SOM to predict the redshift distribution for a set of photometric objects.  We will make a simple cut in spectroscopic redshift to create a compact redshift bin.  In more realistic circumstances we would likely be using color cuts or photometric redshift estimates to define our test bin(s).  We will cut our photometric sample to only include galaxies in 0.5<specz<0.9.\n",
    "\n",
    "We will need to trim both our spec-z set to i<23.5 to match our trained SOM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = '../../src/rail/examples_data/testdata/test_dc2_training_9816.hdf5'\n",
    "data = tables_io.read(testfile)['photometry']\n",
    "mask = ((data['redshift'] > 0.2) & (data['redshift']<0.5))\n",
    "brightmask = ((mask) & (data['mag_i_lsst']<23.5))\n",
    "trim_data = {}\n",
    "bright_data = {}\n",
    "for key in data.keys():\n",
    "    trim_data[key] = data[key][mask]\n",
    "    bright_data[key] = data[key][brightmask]\n",
    "trimdict = dict(photometry=trim_data)\n",
    "brightdict = dict(photometry=bright_data)\n",
    "# add data to data store\n",
    "test_data = DS.add_data(\"tomo_bin\", trimdict, TableHandle)\n",
    "bright_data = DS.add_data(\"bright_bin\", brightdict, TableHandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "specfile = \"../../src/rail/examples_data/testdata/test_dc2_validation_9816.hdf5\"\n",
    "spec_data = tables_io.read(specfile)['photometry']\n",
    "smask = (spec_data['mag_i_lsst'] <23.5)\n",
    "trim_spec = {}\n",
    "for key in spec_data.keys():\n",
    "    trim_spec[key] = spec_data[key][smask]\n",
    "trim_dict = dict(photometry=trim_spec)\n",
    "spec_data = DS.add_data(\"spec_data\", trim_dict, TableHandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a0889",
   "metadata": {},
   "source": [
    "Note that we have removed the 'photometry' group, we will specify the `phot_groupname` as \"\" in the parameters below.<br>\n",
    "As before, let us specify our initialization params for the SomocluSOMSummarizer stage, including:<br>\n",
    "`model`: name of the pickled model that we created, in this case \"output_SOM_model.pkl\"<br>\n",
    "`hdf5_groupname` (str): hdf5 group for our photometric data (in our case \"\")<br>\n",
    "`objid_name` (str): string specifying the name of the ID column, if present photom data, will be written out to cellid_output file<br>\n",
    "`spec_groupname` (str): hdf5 group for the spectroscopic data<br>\n",
    "`nzbins` (int): number of bins to use in our histogram ensemble<br>\n",
    "`n_clusters` (int): number of hierarchical clusters<br>\n",
    "`nsamples` (int): number of bootstrap samples to generate<br>\n",
    "`output` (str): name of the output qp file with N samples<br>\n",
    "`single_NZ` (str): name of the qp file with fiducial distribution<br>\n",
    "`uncovered_cluster_file` (str): name of hdf5 file containing a list of all of the clusters with phot data but no spec-z objects: photometric objects in these cells will *not* be accounted for in the final N(z), and should really be removed from the sample before running the summarizer.  Note that we return a single integer that is constructed from the pairs of SOM cell indices via `np.ravel_multi_index`(indices).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907b8ce",
   "metadata": {},
   "source": [
    "Now let's initialize and run the summarizer.  One feature of the SOM: if any SOM cells contain photometric data but do not contain any redshifts values in the spectroscopic set, then no reasonable redshift estimate for those objects is defined, and they are skipped.  The method currently prints the indices of uncovered cells, we may modify the algorithm to actually output the uncovered galaxies in a separate file in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a53ca9a",
   "metadata": {},
   "source": [
    "Let's open the fiducial N(z) file, plot it, and see how it looks, and compare it to the true tomographic bin file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf19d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_dict = dict(model=\"output_SOMoclu_model.pkl\", hdf5_groupname='photometry',\n",
    "                 spec_groupname='photometry', nzbins=101, nsamples=25,\n",
    "                 output='SOM_ensemble.hdf5', single_NZ='fiducial_SOMoclu_NZ.hdf5',\n",
    "                 uncovered_cluster_file='all_uncovered_clusters.hdf5',\n",
    "                 objid_name='id',\n",
    "                 cellid_output='output_cellIDs.hdf5')\n",
    "som_summarizer = somocluSOMSummarizer.make_stage(name='SOMoclu_summarizer', **summ_dict)    \n",
    "som_summarizer.summarize(test_data, spec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_ens = qp.read(\"fiducial_SOMoclu_NZ.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cont_hist(data, bins):\n",
    "    hist, bin_edge = np.histogram(data, bins=bins, density=True)\n",
    "    return hist, (bin_edge[1:]+bin_edge[:-1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7113c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nz_hist, zbin = get_cont_hist(test_data.data['photometry']['redshift'], np.linspace(0,3,101))\n",
    "som_nz_hist = np.squeeze(fid_ens.pdf(zbin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e830b",
   "metadata": {},
   "source": [
    "Now we try to group SOM cells together with hierarchical clustering method. To do this, we simply specify `n_cluster` in the input dict. We want to test how many hierarchical clusters are optimal for the redshift calibration task. We evaluate the performance by three values: the difference between mean redshifts of the phot and spec catalog; the difference between standard deviations; the ratio between effective number density of represented photometric sources and the whole photometric sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b86db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusterss = np.array([50, 100, 200, 500, 1000, 1500, 2000, 3000, 4000, 71*71])\n",
    "true_full_mean = np.mean(test_data.data['photometry']['redshift'])\n",
    "true_full_std = np.std(test_data.data['photometry']['redshift'])\n",
    "mu_diff = np.zeros(n_clusterss.size)\n",
    "means_diff = np.zeros((n_clusterss.size, 25))\n",
    "\n",
    "std_diff_mean = np.zeros(n_clusterss.size)\n",
    "neff_p_to_neff = np.zeros(n_clusterss.size)\n",
    "std_diff = np.zeros((n_clusterss.size, 25))\n",
    "for i, n_clusters in enumerate(n_clusterss):\n",
    "    summ_dict = dict(model=\"output_SOMoclu_model.pkl\", hdf5_groupname='photometry',\n",
    "                 spec_groupname='photometry', nzbins=101, nsamples=25,\n",
    "                 output='SOM_ensemble.hdf5', single_NZ='fiducial_SOMoclu_NZ.hdf5',\n",
    "                 n_clusters=n_clusters,\n",
    "                 uncovered_cluster_file='all_uncovered_clusters.hdf5',\n",
    "                 objid_name='id',\n",
    "                 cellid_output='output_cellIDs.hdf5')\n",
    "    som_summarizer = somocluSOMSummarizer.make_stage(name='SOMoclu_summarizer', **summ_dict)    \n",
    "    som_summarizer.summarize(test_data, spec_data)\n",
    "    \n",
    "    full_ens = qp.read(\"SOM_ensemble.hdf5\")\n",
    "    full_means = full_ens.mean().flatten()\n",
    "    full_stds = full_ens.std().flatten()\n",
    "    \n",
    "    # mean and width of bootstraps\n",
    "    mu_diff[i] = np.mean(full_means) - true_full_mean\n",
    "    means_diff[i] = full_means - true_full_mean\n",
    "    \n",
    "    std_diff_mean[i] = np.mean(full_stds) - true_full_std\n",
    "    std_diff[i] = full_stds - true_full_std\n",
    "    neff_p_to_neff[i] = som_summarizer.neff_p_to_neff\n",
    "    full_sig = np.std(full_means)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(20,5))\n",
    "\n",
    "for i in range(25):\n",
    "    axes[0].plot(n_clusterss, means_diff.T[i], lw=0.2, color='C1')\n",
    "axes[0].plot(n_clusterss, mu_diff, lw=1, color='k')\n",
    "axes[0].axhline(0,1,0)\n",
    "axes[0].set_xlabel('Number of clusters')\n",
    "axes[0].set_ylabel(r'$\\left\\langle z \\right\\rangle - \\left\\langle z \\right\\rangle_{\\mathrm{true}}$')\n",
    "\n",
    "for i in range(25):\n",
    "    axes[1].plot(n_clusterss, std_diff.T[i], lw=0.2, color='C1')\n",
    "axes[1].plot(n_clusterss, std_diff_mean, lw=1, color='k')\n",
    "axes[1].axhline(0,1,0)\n",
    "\n",
    "axes[1].set_xlabel('Number of clusters')\n",
    "axes[1].set_ylabel(r'$\\mathrm{std}(z) - \\mathrm{std}(z)_{\\mathrm{true}}$')\n",
    "\n",
    "\n",
    "axes[2].plot(n_clusterss, neff_p_to_neff*100, lw=1, color='k')\n",
    "\n",
    "axes[2].set_xlabel('Number of clusters')\n",
    "axes[2].set_ylabel(r'$n_{\\mathrm{eff}}\\'/n_{\\mathrm{eff}}$(%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5403c8",
   "metadata": {},
   "source": [
    "From the three plots above, we can see that when n_cluster>1000, the redshift bias is within ~0.01 and the difference in standard deviation does not change significantly, but the effective number density continues to decrease. This is because when we have more clusters, the risk that a cluster does not contain a spectroscopic source becomes higher. Therefore, we might choose ~1000 clusters for the calibration in this practice, so that we can keep as many galaxies as possible while minimize the bias in average and standard deviation of galaxy redshifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7088c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_dict = dict(model=\"output_SOMoclu_model.pkl\", hdf5_groupname='photometry',\n",
    "                 spec_groupname='photometry', nzbins=101, nsamples=25,\n",
    "                 output='SOM_ensemble.hdf5', single_NZ='fiducial_SOMoclu_NZ.hdf5',\n",
    "                 n_clusters=1000,\n",
    "                 uncovered_cluster_file='all_uncovered_clusters.hdf5',\n",
    "                 objid_name='id',\n",
    "                 cellid_output='output_cellIDs.hdf5')\n",
    "\n",
    "som_summarizer = somocluSOMSummarizer.make_stage(name='SOMoclu_summarizer', **summ_dict)\n",
    "som_summarizer.summarize(test_data, spec_data)\n",
    "\n",
    "test_nz_hist, zbin = get_cont_hist(test_data.data['photometry']['redshift'], np.linspace(0,3,101))\n",
    "som_nz_hist = np.squeeze(fid_ens.pdf(zbin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12,8))\n",
    "ax.set_xlabel(\"redshift\", fontsize=15)\n",
    "ax.set_ylabel(\"N(z)\", fontsize=15)\n",
    "ax.plot(zbin, test_nz_hist, label='True N(z)')\n",
    "ax.plot(zbin, som_nz_hist, label='SOM N(z)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8da0f1",
   "metadata": {},
   "source": [
    "Seems fine, roughly the correct redshift range for the lower redshift peak, but a few secondary peaks at large z tail.  What if we try the bright dataset that we made?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b65232",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_dict = dict(model=\"output_SOMoclu_model.pkl\", hdf5_groupname='photometry',\n",
    "                   spec_groupname='photometry', nzbins=101, nsamples=25,\n",
    "                   output='BRIGHT_SOMoclu_ensemble.hdf5', single_NZ='BRIGHT_fiducial_SOMoclu_NZ.hdf5',\n",
    "                   uncovered_cluster_file=\"BRIGHT_uncovered_clusters.hdf5\",\n",
    "                   n_clusters=1000,\n",
    "                   objid_name='id',\n",
    "                   cellid_output='BRIGHT_output_cellIDs.hdf5')\n",
    "bright_summarizer = somocluSOMSummarizer.make_stage(name='bright_summarizer', **bright_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2675f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_summarizer.summarize(bright_data, spec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa30b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_fid_ens = qp.read(\"BRIGHT_fiducial_SOMoclu_NZ.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a42de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_nz_hist, zbin = get_cont_hist(bright_data.data['photometry']['redshift'], np.linspace(0,3,101))\n",
    "bright_som_nz_hist = np.squeeze(bright_fid_ens.pdf(zbin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12,8))\n",
    "ax.set_xlabel(\"redshift\", fontsize=15)\n",
    "ax.set_ylabel(\"N(z)\", fontsize=15)\n",
    "ax.plot(zbin, bright_nz_hist, label='True N(z), bright')\n",
    "ax.plot(zbin, bright_som_nz_hist, label='SOM N(z), bright')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825f0c2",
   "metadata": {},
   "source": [
    "Looks better, we've eliminated the secondary peak. Now, SOMs are a bit touchy to train, and are highly dependent on the dataset used to train them.  This demo used a relatively small dataset (~150,000 DC2 galaxies from one healpix pixel) to train the SOM, and even smaller photometric and spectroscopic datasets of 10,000 and 20,000 galaxies.  We should expect slightly better results with more data, at least in cells where the spectroscopic data is representative.\n",
    "\n",
    "However, there is a caveat that SOMs are not guaranteed to converge, and are very sensitive to both the input data and tunable parameters of the model.  So, users should do some verification tests before trusting the SOM is going to give accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a400614",
   "metadata": {},
   "source": [
    "Finally, let's load up our bootstrap ensembles and overplot N(z) of bootstrap samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c5ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_ens = qp.read(\"BRIGHT_SOMoclu_ensemble.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(1,1,figsize=(12, 8))\n",
    "ax.set_xlim((0,1))\n",
    "ax.set_xlabel(\"redshift\", fontsize=15)\n",
    "ax.set_ylabel(\"bootstrap N(z)\", fontsize=15)\n",
    "ax.legend(loc='upper right', fontsize=13);\n",
    "\n",
    "ax.plot(zbin, bright_nz_hist, label='True N(z), bright', color='C1', zorder=1)\n",
    "ax.plot(zbin, bright_som_nz_hist, label='SOM mean N(z), bright', color='k', zorder=2)\n",
    "\n",
    "for i in range(boot_ens.npdf):\n",
    "    #ax = plt.subplot(2,3,i+1)\n",
    "    pdf = np.squeeze(boot_ens[i].pdf(zbin))\n",
    "    if i == 0:        \n",
    "        ax.plot(zbin, pdf, color='C2',zorder=0, alpha=0.5, label='SOM bootstrap N(z) samples, bright')\n",
    "    else:\n",
    "        ax.plot(zbin, pdf, color='C2',zorder=0, alpha=0.5)\n",
    "    #boot_ens[i].plot_native(axes=ax, label=f'SOM bootstrap {i}')\n",
    "plt.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c2f84",
   "metadata": {},
   "source": [
    "# quantitative metrics\n",
    "\n",
    "Let's look at how we've done at estimating the mean redshift and \"width\" (via standard deviation) of our tomographic bin compared to the true redshift and \"width\" for both our \"full\" sample and \"bright\" i<23.5 samples.  We will plot the mean and std dev for the full and bright distributions compared to the true mean and width, and show the Gaussian uncertainty approximation given the scatter in the bootstraps for the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ens = qp.read(\"SOM_ensemble.hdf5\")\n",
    "full_means = full_ens.mean().flatten()\n",
    "full_stds = full_ens.std().flatten()\n",
    "true_full_mean = np.mean(test_data.data['photometry']['redshift'])\n",
    "true_full_std = np.std(test_data.data['photometry']['redshift'])\n",
    "# mean and width of bootstraps\n",
    "full_mu = np.mean(full_means)\n",
    "full_sig = np.std(full_means)\n",
    "full_norm = norm(loc=full_mu, scale=full_sig)\n",
    "grid = np.linspace(0, .7, 301)\n",
    "full_uncert = full_norm.pdf(grid)*2.51*full_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab40ed",
   "metadata": {},
   "source": [
    "Let's check the accuracy and precision of mean readshift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bcef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean redshift of the SOM ensemble is: \"+str(round(np.mean(full_means),4)) + '+-' + str(round(np.std(full_means),4)))\n",
    "print(\"The mean redshift of the real data is: \"+str(round(true_full_mean,4)))\n",
    "print(\"The bias of mean redshift is:\"+str(round(np.mean(full_means)-true_full_mean,4)) + '+-' + str(round(np.std(full_means),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_means = boot_ens.mean().flatten()\n",
    "bright_stds = boot_ens.std().flatten()\n",
    "true_bright_mean = np.mean(bright_data.data['photometry']['redshift'])\n",
    "true_bright_std = np.std(bright_data.data['photometry']['redshift'])\n",
    "bright_uncert = np.std(bright_means)\n",
    "# mean and width of bootstraps\n",
    "bright_mu = np.mean(bright_means)\n",
    "bright_sig = np.std(bright_means)\n",
    "bright_norm = norm(loc=bright_mu, scale=bright_sig)\n",
    "bright_uncert = bright_norm.pdf(grid)*2.51*bright_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4265a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean redshift of the SOM ensemble is: \"+str(round(np.mean(bright_means),4)) + '+-' + str(round(np.std(bright_means),4)))\n",
    "print(\"The mean redshift of the real data is: \"+str(round(true_bright_mean,4)))\n",
    "print(\"The bias of mean redshift is:\"+str(round(np.mean(bright_means)-true_bright_mean, 4)) + '+-' + str(round(np.std(bright_means),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,18))\n",
    "ax0 = plt.subplot(2, 1, 1)\n",
    "ax0.set_xlim(0.0, 0.7)\n",
    "ax0.axvline(true_full_mean, color='r', lw=3, label='true mean full sample')\n",
    "ax0.vlines(full_means, ymin=0, ymax=1, color='r', ls='--', lw=1, label='bootstrap means')\n",
    "ax0.axvline(true_full_std, color='b', lw=3, label='true std full sample')\n",
    "ax0.vlines(full_stds, ymin=0, ymax=1, lw=1, color='b', ls='--', label='bootstrap stds')\n",
    "ax0.plot(grid, full_uncert, c='k', label='full mean uncertainty')\n",
    "ax0.legend(loc='upper right', fontsize=12)\n",
    "ax0.set_xlabel('redshift', fontsize=12)\n",
    "ax0.set_title('mean and std for full sample', fontsize=12)\n",
    "\n",
    "ax1 = plt.subplot(2, 1, 2)\n",
    "ax1.set_xlim(0.0, 0.7)\n",
    "ax1.axvline(true_bright_mean, color='r', lw=3, label='true mean bright sample')\n",
    "ax1.vlines(bright_means, ymin=0, ymax=1, color='r', ls='--', lw=1, label='bootstrap means')\n",
    "ax1.axvline(true_bright_std, color='b', lw=3, label='true std bright sample')\n",
    "ax1.plot(grid, bright_uncert, c='k', label='bright mean uncertainty')\n",
    "ax1.vlines(bright_stds, ymin=0, ymax=1, ls='--', lw=1, color='b', label='bootstrap stds')\n",
    "ax1.legend(loc='upper right', fontsize=12)\n",
    "ax1.set_xlabel('redshift', fontsize=12)\n",
    "ax1.set_title('mean and std for bright sample', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f5e0c0",
   "metadata": {},
   "source": [
    "For both cases, the mean redshifts seem to be pretty precise and accurate (bright sample seems more precise). For the full sample, the SOM N(z) are slightly wider, while for the bright sample the widths are also fairly accurate.\n",
    "For both cases, the errors in mean redshift are at levels of ~0.005, close to the tolerance for cosmological analysis. However, we have not consider the photometric error in magnitudes and colors, as well as additional color selections. Our sample is also limited. This demo only serves as a preliminary implementation of SOM in RAIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13200194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
