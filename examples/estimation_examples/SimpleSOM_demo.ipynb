{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08afaafe",
   "metadata": {},
   "source": [
    "# SimpleSOMSummarizer demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f57f69",
   "metadata": {},
   "source": [
    "Author: Sam Schmidt <br>\n",
    "Last successfully run: April 26, 2023<br>\n",
    "\n",
    "This notebook shows a quick demonstration of the use of the `SimpleSOMSummarizer` summarization module.  Algorithmically, this module is not very different from the NZDir estimator/summarizer.  NZDir operates by finding neighboring photometric points around spectroscopic objects.  SimpleSOMSummarizer takes a large training set of data in the `Inform_SimpleSOMSUmmarizer` stage and trains a self-organized map (SOM) (using code from the `minisom` package available at: https://github.com/JustGlowing/minisom).  Once the SOM is set up, the \"winning\"/best-fit cells are determined for both the photometric/unknown data and a set of spectroscopic data with known redshifts.  For each SOM cell, the algorithm constructs a histogram using the spectroscopic members mapped to that cell, and weights these by the number of photometric galaxies in that cell.  Both the photometric and spectroscopic datasets can also employ an optional weight per-galaxy. <br>\n",
    "\n",
    "The summarizer also identifies SOM cells that contain photometric data but do not contain and galaxies with a measured spec-z, and thus do not have an obvious redshift estimate.  It writes out the (raveled) SOM cell indices that contain \"uncovered\"/uncalibratable data to the file specified by the `uncovered_cell_file` option as a list of integers.  The cellIDs and galaxy/objIDs for all photometric galaxies will be written out to the file specified by the `cellid_output` parameter.  Any galaxies in these cells should really be removed, and thus some iteration may be necessary in defining bin membership by looking at the properties of objects in these uncovered cells before a final N(z) is estimated, as otherwise a bias may be present.<br>\n",
    "\n",
    "The shape and number of cells used in constructing the SOM affects performance, as do several tuning parameters.  This paper, http://www.giscience2010.org/pdfs/paper_230.pdf gives a rough guideline that the number of cells should be of the order ~ 5 x sqrt (number of data rows x number of column rows), though this is a rough guide.  Some studies have found a 2D SOM that is more elongated in one direction to be preferential, while others claim that a square layout is optimal, the user can set the number of cells in each SOM dimension via the `n_dim` and `m_dim` parameters.  For more discussion on SOMs see the Appendices of this KiDS paper:  http://arxiv.org/abs/1909.09632.\n",
    "\n",
    "As with the other RAIL summarizers, we bootstrap the spectroscopic sample and return N bootstraps in an ensemble, along with a single fiducial N(z) estimate.<br>\n",
    "\n",
    "More specific details of the algorithm's set up will be described in the course of this notebook, along with some illustrative plots.\n",
    "\n",
    "Let's set up our dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2971b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import rail\n",
    "import qp\n",
    "import os\n",
    "import tables_io\n",
    "from rail.core.data import TableHandle\n",
    "from rail.core.stage import RailStage\n",
    "from minisom import MiniSom\n",
    "from rail.core.utils import RAILDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab63889",
   "metadata": {},
   "source": [
    "Next, let's set up the Data Store, so that our RAIL module will know where to fetch data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e6136",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, let's grab some data files.  For the SOM, we will want to train on a fairly large, representative set that encompasses all of our expected data.  We'll grab a larger data file than we typically use in our demos to ensure that we construct a meaningful SOM.\n",
    "\n",
    "This data consists of ~150,000 galaxies from a single healpix pixel of the comsoDC2 truth catalog with mock 10-year magnitude errors added.  It is cut at a relatively bright i<23.5 magnitudes in order to concentrate on galaxies with particularly high S/N rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9259d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = \"./healpix_10326_bright_data.hdf5\"\n",
    "\n",
    "if not os.path.exists(training_file):\n",
    "  os.system('curl -O https://portal.nersc.gov/cfs/lsst/schmidt9/healpix_10326_bright_data.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# way to get big data file\n",
    "training_file = \"./healpix_10326_bright_data.hdf5\"\n",
    "training_data = DS.read_file(\"training_data\", TableHandle, training_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415231fc",
   "metadata": {},
   "source": [
    "Now, let's set up the inform stage for our summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeca17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.estimation.algos.simpleSOM import Inform_SimpleSOMSummarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f1b0b",
   "metadata": {},
   "source": [
    "We need to define all of our necessary initialization params, which includes the following:<br>\n",
    "`name` (str): the name of our estimator, as utilized by ceci<br>\n",
    "`model` (str): the name for the model file containing the SOM and associated parameters that will be written by this stage<br>\n",
    "`hdf5_groupname` (str): name of the hdf5 group (if any) where the photometric data resides in the training file<br>\n",
    "`n_dim` (int): the number of dimensions in the x-direction for our 2D SOM<br>\n",
    "`m_dim` (int): the number of dimensions in the y-direction for our 2D SOM<br>\n",
    "`som_iterations` (int): the number of iteration steps during SOM training.  SOMs can take a while to converge, so we will use a fairly large number of 500,000 iterations.<br>\n",
    "`som_sigma` (float): the \"radius\" of how far to spread changes in the SOM <br>\n",
    "`som_learning_rate` (float): a number between 0 and 1 that controls how quickly the weighting function decreases.  SOM's are not guaranteed to converge mathematically, and so this parameter tunes how the response drops per iteration.  A typical values we might use might be between 0.5 and 0.75.<br>\n",
    "`column_usage` (str):  this value determines what values will be used to construct the SOM, valid choices are `colors`, `magandcolors`, and `columns`.  If set to `colors`, the code will take adjacent columns as specified in `usecols` to construct colors and use those as SOM inputs.  If set to `magandcolors` it will use the single column specfied by `ref_column_name` and the aforementioned colors to construct the SOM.  If set to `columns` then it will simply take each of the columns in `usecols` with no modification.  So, if a user wants to use K magnitudes and L colors, they can precompute the colors and specify all names in `usecols`.  NOTE: accompanying `usecols` you must have a `nondetect_val` dictionary that lists the replacement values for any non-detection-valued entries for each column, see the code for an example dictionary.  WE will set `column_usage` to colors and use only colors in this example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_dict = dict(model='output_SOM_model.pkl', hdf5_groupname='photometry',\n",
    "                   n_dim=71, m_dim=71, som_iterations=500_000,\n",
    "                   som_sigma=12.0, som_learning_rate=0.75,\n",
    "                   column_usage='colors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db96d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_som = Inform_SimpleSOMSummarizer.make_stage(name='inform_som', **inform_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8d42d",
   "metadata": {},
   "source": [
    "Let's run our stage, which will write out a file called `output_SOM_model.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a342e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inform_som.inform(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554ddce",
   "metadata": {},
   "source": [
    "Running the stage took ~4 minutes wall time on a desktop Mac and ~5 minutes on a Mac laptop.  Remember, however, that in many production cases we would likely load a pre-trained SOM specifically tuned to the given dataset, and this inform stage would not be run each time.<br>\n",
    "Let's read in the SOM model file, which contains our som model and several of the parameters used in constructing the SOM, and needed by our summarization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932fd583",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_SOM_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4388e3",
   "metadata": {},
   "source": [
    "To visualize our SOM, let's calculate the cell occupation of our training sample, as well as the mean redshift of the galaxies in each cell.  The SOM took colors as inputs, so we will need to construct the colors for our training set galaxie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ['u','g','r','i','z','y']\n",
    "bandnames = [f\"mag_{band}_lsst\" for band in bands]\n",
    "ngal = len(training_data.data['photometry']['mag_i_lsst'])\n",
    "colors = np.zeros([5, ngal])\n",
    "for i in range(5):\n",
    "    colors[i] = training_data.data['photometry'][bandnames[i]] - training_data.data['photometry'][bandnames[i+1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbc068",
   "metadata": {},
   "source": [
    "We can calculate the best SOM cell using the SOM.winner() method from minisom, which will return the 2D SOM coordinates for each galaxy, and then use these for our visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63709f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOM = model['som']\n",
    "winner_coordinates = np.array([SOM.winner(x) for x in colors.T]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22980ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim=71\n",
    "m_dim=71\n",
    "meanszs = np.zeros((n_dim,m_dim))\n",
    "cellocc = np.zeros((n_dim,m_dim))\n",
    "for i in range(n_dim):\n",
    "    for j in range(m_dim):\n",
    "        mask = ((winner_coordinates[0] == i) & (winner_coordinates[1]==j))\n",
    "        szs = training_data.data['photometry']['redshift'][mask]\n",
    "        if np.sum(mask)>0:\n",
    "            meanszs[i,j] = np.mean(szs)\n",
    "            cellocc[i,j] = len (szs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566ba96",
   "metadata": {},
   "source": [
    "Here is the cell occupation distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b54886",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(cellocc.T,cmap='jet')\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(label='cell occupation', size=15)\n",
    "plt.clim((0,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d4bd3e",
   "metadata": {},
   "source": [
    "And here is the mean redshift per cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e7b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(meanszs.T,cmap='jet')\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(label='mean redshift', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ca71b",
   "metadata": {},
   "source": [
    "Note that there is spatial correlation between redshift and cell position, which is good, this is showing how there are gradual changes in redshift between similarly-colored galaxies (and sometimes abrupt changes, when degeneracies are present)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474dcf0",
   "metadata": {},
   "source": [
    "Now that we have illustrated what exactly we have constructed, let's use the SOM to predict the redshift distribution for a set of photometric objects.  We will make a simple cut in spectroscopic redshift to create a compact redshift bin.  In more realistic circumstances we would likely be using color cuts or photometric redshift estimates to define our test bin(s).  We will cut our photometric sample to only include galaxies in 0.5<specz<0.9.\n",
    "\n",
    "We will need to trim both our spec-z set to i<23.5 to match our trained SOM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = os.path.join(RAILDIR, 'rail/examples_data/testdata/test_dc2_training_9816.hdf5')\n",
    "data = tables_io.read(testfile)['photometry']\n",
    "mask = ((data['redshift'] > 0.2) & (data['redshift']<0.5))\n",
    "brightmask = ((mask) & (data['mag_i_lsst']<23.5))\n",
    "trim_data = {}\n",
    "bright_data = {}\n",
    "for key in data.keys():\n",
    "    trim_data[key] = data[key][mask]\n",
    "    bright_data[key] = data[key][brightmask]\n",
    "trimdict = dict(photometry=trim_data)\n",
    "brightdict = dict(photometry=bright_data)\n",
    "# add data to data store\n",
    "test_data = DS.add_data(\"tomo_bin\", trimdict, TableHandle)\n",
    "bright_data = DS.add_data(\"bright_bin\", brightdict, TableHandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "specfile = os.path.join(RAILDIR, \"rail/examples_data/testdata/test_dc2_validation_9816.hdf5\")\n",
    "spec_data = tables_io.read(specfile)['photometry']\n",
    "smask = (spec_data['mag_i_lsst'] <23.5)\n",
    "trim_spec = {}\n",
    "for key in spec_data.keys():\n",
    "    trim_spec[key] = spec_data[key][smask]\n",
    "trim_dict = dict(photometry=trim_spec)\n",
    "spec_data = DS.add_data(\"spec_data\", trim_dict, TableHandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47fe3f",
   "metadata": {},
   "source": [
    "Note that we have removed the 'photometry' group, we will specify the `phot_groupname` as \"\" in the parameters below.<br>\n",
    "As before, let us specify our initialization params for the SimpleSOMSummarizer stage, including:<br>\n",
    "`model`: name of the pickled model that we created, in this case \"output_SOM_model.pkl\"<br>\n",
    "`hdf5_groupname` (str): hdf5 group for our photometric data (in our case \"\")<br>\n",
    "`objid_name` (str): string specifying the name of the ID column, if present photom data, will be written out to cellid_output file<br>\n",
    "`spec_groupname` (str): hdf5 group for the spectroscopic data<br>\n",
    "`nzbins` (int): number of bins to use in our histogram ensemble<br>\n",
    "`nsamples` (int): number of bootstrap samples to generate<br>\n",
    "`output` (str): name of the output qp file with N samples<br>\n",
    "`single_NZ` (str): name of the qp file with fiducial distribution<br>\n",
    "`uncovered_cell_file` (str): name of hdf5 file containing a list of all of the cells with phot data but no spec-z objects: photometric objects in these cells will *not* be accounted for in the final N(z), and should really be removed from the sample before running the summarizer.  Note that we return a single integer that is constructed from the pairs of SOM cell indices via `np.ravel_multi_index`(indices).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65461a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_dict = dict(model=\"output_SOM_model.pkl\", hdf5_groupname='photometry',\n",
    "                 spec_groupname='photometry', nzbins=101, nsamples=25,\n",
    "                 output='SOM_ensemble.hdf5', single_NZ='fiducial_SOM_NZ.hdf5',\n",
    "                 uncovered_cell_file='all_uncovered_cells.hdf5',\n",
    "                 objid_name='id',\n",
    "                 cellid_output='output_cellIDs.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae391a6",
   "metadata": {},
   "source": [
    "Now let's initialize and run the summarizer.  One feature of the SOM: if any SOM cells contain photometric data but do not contain any redshifts values in the spectroscopic set, then no reasonable redshift estimate for those objects is defined, and they are skipped.  The method currently prints the indices of uncovered cells, we may modify the algorithm to actually output the uncovered galaxies in a separate file in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64993ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.estimation.algos.simpleSOM import SimpleSOMSummarizer\n",
    "som_summarizer = SimpleSOMSummarizer.make_stage(name='SOM_summarizer', **summ_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05dd94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_summarizer.summarize(test_data, spec_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd04a30",
   "metadata": {},
   "source": [
    "Let's open the fiducial N(z) file, plot it, and see how it looks, and compare it to the true tomographic bin file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_ens = qp.read(\"fiducial_SOM_NZ.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(25,8))\n",
    "fid_ens.plot_native(axes=axs[0],label=\"SOM estimate\")\n",
    "axs[0].set_xlabel(\"redshift\", fontsize=15)\n",
    "axs[0].set_ylabel(\"N(z)\", fontsize=15)\n",
    "axs[1].hist(test_data.data['photometry']['redshift'], bins=np.linspace(0,3,101));\n",
    "axs[1].set_xlabel(\"redshift\", fontsize=15)\n",
    "axs[1].set_ylabel(\"N(z)\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b1024",
   "metadata": {},
   "source": [
    "Not great, roughly the correct redshift range for the lower redshift peak, but a large secondary peak at ~1.0<z<1.5.  What if we try the bright dataset that we made?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dac337",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_dict = dict(model=\"output_SOM_model.pkl\", hdf5_groupname='photometry',\n",
    "                   spec_groupname='photometry', nzbins=101, nsamples=25,\n",
    "                   output='BRIGHT_SOM_ensemble.hdf5', single_NZ='BRIGHT_fiducial_SOM_NZ.hdf5',\n",
    "                   uncovered_cell_file=\"BRIGHT_uncovered_cells.hdf5\",\n",
    "                   objid_name='id',\n",
    "                   cellid_output='BRIGHT_output_cellIDs.hdf5')\n",
    "bright_summarizer = SimpleSOMSummarizer.make_stage(name='bright_summarizer', **bright_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2911b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_summarizer.summarize(bright_data, spec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e594c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_fid_ens = qp.read(\"BRIGHT_fiducial_SOM_NZ.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(25,8))\n",
    "bright_fid_ens.plot_native(axes=axs[0],label=\"SOM estimate\")\n",
    "axs[0].set_xlabel(\"redshift\", fontsize=15)\n",
    "axs[0].set_ylabel(\"N(z)\", fontsize=15)\n",
    "axs[0].legend(loc='upper right', fontsize=15)\n",
    "axs[1].hist(bright_data.data['photometry']['redshift'], bins=np.linspace(0,3,101),label='true N(z)');\n",
    "axs[1].set_xlabel(\"redshift\", fontsize=15)\n",
    "axs[1].set_ylabel(\"N(z)\", fontsize=15)\n",
    "axs[1].legend(loc='upper right', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c7ec0",
   "metadata": {},
   "source": [
    "Slightly better, we've eliminated the secondary peak.  Now, SOMs are a bit touchy to train, and are highly dependent on the dataset used to train them.  This demo used a relatively small dataset (~150,000 DC2 galaxies from one healpix pixel) to train the SOM, and even smaller photometric and spectroscopic datasets of 10,000 and 20,000 galaxies.  We should expect slightly better results with more data, at least in cells where the spectroscopic data is representative.\n",
    "\n",
    "However, there is a caveat that SOMs are not guaranteed to converge, and are very sensitive to both the input data and tunable parameters of the model.  So, users should do some verification tests before trusting the SOM is going to give accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa8ea7",
   "metadata": {},
   "source": [
    "Finally, let's load up our bootstrap ensembles and print out the first six to see what kind of variation we see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_ens = qp.read(\"BRIGHT_SOM_ensemble.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(20,15))\n",
    "for i in range(6):\n",
    "    ax = plt.subplot(2,3,i+1)\n",
    "    ax.set_xlim((0,3))\n",
    "    boot_ens[i].plot_native(axes=ax, label=f'SOM bootstrap {i}')\n",
    "    ax.set_xlabel(\"redshift\", fontsize=15)\n",
    "    ax.set_ylabel(\"bootstrap N(z)\", fontsize=15)\n",
    "    ax.legend(loc='upper right', fontsize=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d895a428",
   "metadata": {},
   "source": [
    "# quantitative metrics\n",
    "\n",
    "Let's look at how we've done at estimating the mean redshift and \"width\" (via standard deviation) of our tomographic bin compared to the true redshift and \"width\" for both our \"full\" sample and \"bright\" i<23.5 samples.  We will plot the mean and std dev for the full and bright distributions compared to the true mean and width, and show the Gaussian uncertainty approximation given the scatter in the bootstraps for the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad80597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ens = qp.read(\"SOM_ensemble.hdf5\")\n",
    "full_means = full_ens.mean().flatten()\n",
    "full_stds = full_ens.std().flatten()\n",
    "true_full_mean = np.mean(test_data.data['photometry']['redshift'])\n",
    "true_full_std = np.std(test_data.data['photometry']['redshift'])\n",
    "# mean and width of bootstraps\n",
    "full_mu = np.mean(full_means)\n",
    "full_sig = np.std(full_means)\n",
    "full_norm = norm(loc=full_mu, scale=full_sig)\n",
    "grid = np.linspace(0, .7, 301)\n",
    "full_uncert = full_norm.pdf(grid)*2.51*full_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76048f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_means = boot_ens.mean().flatten()\n",
    "bright_stds = boot_ens.std().flatten()\n",
    "true_bright_mean = np.mean(bright_data.data['photometry']['redshift'])\n",
    "true_bright_std = np.std(bright_data.data['photometry']['redshift'])\n",
    "bright_uncert = np.std(bright_means)\n",
    "# mean and width of bootstraps\n",
    "bright_mu = np.mean(bright_means)\n",
    "bright_sig = np.std(bright_means)\n",
    "bright_norm = norm(loc=bright_mu, scale=bright_sig)\n",
    "bright_uncert = bright_norm.pdf(grid)*2.51*bright_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,18))\n",
    "ax0 = plt.subplot(2, 1, 1)\n",
    "ax0.set_xlim(0.0, 0.7)\n",
    "ax0.axvline(true_full_mean, color='r', lw=3, label='true mean full sample')\n",
    "ax0.vlines(full_means, ymin=0, ymax=1, color='r', ls='--', lw=1, label='bootstrap means')\n",
    "ax0.axvline(true_full_std, color='b', lw=3, label='true std full sample')\n",
    "ax0.vlines(full_stds, ymin=0, ymax=1, lw=1, color='b', ls='--', label='bootstrap stds')\n",
    "ax0.plot(grid, full_uncert, c='k', label='full mean uncertainty')\n",
    "ax0.legend(loc='upper right', fontsize=12)\n",
    "ax0.set_xlabel('redshift', fontsize=12)\n",
    "ax0.set_title('mean and std for full sample', fontsize=12)\n",
    "\n",
    "ax1 = plt.subplot(2, 1, 2)\n",
    "ax1.set_xlim(0.0, 0.7)\n",
    "ax1.axvline(true_bright_mean, color='r', lw=3, label='true mean bright sample')\n",
    "ax1.vlines(bright_means, ymin=0, ymax=1, color='r', ls='--', lw=1, label='bootstrap means')\n",
    "ax1.axvline(true_bright_std, color='b', lw=3, label='true std bright sample')\n",
    "ax1.plot(grid, bright_uncert, c='k', label='bright mean uncertainty')\n",
    "ax1.vlines(bright_stds, ymin=0, ymax=1, ls='--', lw=1, color='b', label='bootstrap stds')\n",
    "ax1.legend(loc='upper right', fontsize=12)\n",
    "ax1.set_xlabel('redshift', fontsize=12)\n",
    "ax1.set_title('mean and std for bright sample', fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312db0b",
   "metadata": {},
   "source": [
    "We see that the mean (red) and std dev (blue) estimates are quite biased compared to the truth in both cases, this is not unexpected for a simple true redshift cut and small samples sizes used in this demo.  The std dev estimate is much closer to the truth for the bright sample due to the elimination of the false secondary redshift peak.  However, we are still well above tolerances for any cosmological analysis, actual tomographic samples will need both tight color selections and larger training sets than those used in this demo in order to properly calibrate our redshift distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba685ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
