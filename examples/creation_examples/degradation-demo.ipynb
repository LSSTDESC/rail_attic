{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Engines and Degraders to Generate Galaxy Samples with Errors and Biases\n",
    "\n",
    "author: John Franklin Crenshaw, Sam Schmidt, Eric Charles, others...\n",
    "\n",
    "last run successfully: April 26, 2023\n",
    "\n",
    "This notebook demonstrates how to use a RAIL Engines to create galaxy samples, and how to use Degraders to add various errors and biases to the sample.\n",
    "\n",
    "Note that in the parlance of the Creation Module, \"degradation\" is any post-processing that occurs to the \"true\" sample generated by the Engine. This can include adding photometric errors, applying quality cuts, introducing systematic biases, etc.\n",
    "\n",
    "In this notebook, we will first learn how to draw samples from a RAIL Engine object.\n",
    "Then we will demonstrate how to use the following RAIL Degraders:\n",
    "1. [**LSSTErrorModel**](#LSSTErrorModel), which adds photometric errors\n",
    "2. [**QuantityCut**](#QuantityCut), which applies cuts to the specified columns of the sample\n",
    "3. [**InvRedshiftIncompleteness**](#InvRedshiftIncompleteness), which introduces sample incompleteness\n",
    "4. [**LineConfusion**](#LineConfusion), which introduces spectroscopic errors\n",
    "\n",
    "Throughout the notebook, we will show how you can chain all these Degraders together to build a more complicated degrader.\n",
    "Hopefully, this will allow you to see how you can build your own degrader.\n",
    "\n",
    "*Note on generating redshift posteriors*: regardless of what Degraders you apply, when you use a Creator to estimate posteriors, the posteriors will *always* be calculated with respect to the \"true\" distribution. This is the whole point of the Creation Module -- you can generate degraded samples for which we still have access to the *true* posteriors. For an example of how to calculate posteriors, see `posterior-demo.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pzflow.examples import get_example_flow\n",
    "from rail.creation.engines.flowEngine import FlowCreator\n",
    "from rail.creation.degradation import (\n",
    "    InvRedshiftIncompleteness,\n",
    "    LineConfusion,\n",
    "    LSSTErrorModel,\n",
    "    QuantityCut,\n",
    ")\n",
    "from rail.core.stage import RailStage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the path to the pretrained 'pzflow' used to generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pzflow\n",
    "import os\n",
    "flow_file = os.path.join(os.path.dirname(pzflow.__file__), 'example_files', 'example-flow.pzflow.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by setting up the Rail data store.  RAIL uses [ceci](https://github.com/LSSTDESC/ceci), which is designed for pipelines rather than interactive notebooks, the data store will work around that and enable us to use data interactively.  See the `rail/examples/goldenspike/goldenspike.ipynb` example notebook for more details on the Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"True\" Engine\n",
    "\n",
    "First, let's make an Engine that has no degradation. We can use it to generate a \"true\" sample, to which we can compare all the degraded samples below.\n",
    "\n",
    "Note: in this example, we will use a normalizing flow engine from the [pzflow](https://github.com/jfcrenshaw/pzflow) package. However, everything in this notebook is totally agnostic to what the underlying engine is.\n",
    "\n",
    "The Engine is a type of RailStage object, so we can make one using the `RailStage.make_stage` function for the class of Engine that we want.  We then pass in the configuration parameters as arguments to `make_stage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = int(1e5)\n",
    "flowCreator_truth = FlowCreator.make_stage(name='truth', model=flow_file, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check that the Engine correctly read the underlying PZ Flow object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowCreator_truth.get_data('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we invoke the `sample` method to generate some samples\n",
    "\n",
    "Note that this will return a `DataHandle` object, which can keep both the data itself, and also the path to where the data is written.  When talking to rail stages we can use this as though it were the underlying data and pass it as an argument.  This allows the rail stages to keep track of where their inputs are coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_truth = flowCreator_truth.sample(n_samples, seed=0)\n",
    "print(samples_truth())\n",
    "print(\"Data was written to \", samples_truth.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"LSSTErrorModel\"></a>\n",
    "## Degrader 1: LSSTErrorModel\n",
    "\n",
    "Now, we will demonstrate the `LSSTErrorModel`, which adds photometric errors using a model similar to the model from [Ivezic et al. 2019](https://arxiv.org/abs/0805.2366) (specifically, it uses the model from this paper, without making the high SNR assumption. To restore this assumption and therefore use the exact model from the paper, set `highSNR=True`.)\n",
    "\n",
    "Let's create an error model with the default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorModel = LSSTErrorModel.make_stage(name='error_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the details of the model, including the default settings we are using, you can just print the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add this error model as a degrader and draw some samples with photometric errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_w_errs = errorModel(samples_truth)\n",
    "samples_w_errs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice some of the magnitudes are NaN's. These are non-detections. This means those observed magnitudes were beyond the 30mag limit that is default in `LSSTErrorModel`. \n",
    "You can change this limit and the corresponding flag by setting `magLim=...` and `ndFlag=...` in the constructor for `LSSTErrorModel`. \n",
    "\n",
    "Let's plot the error as a function of magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4), dpi=100)\n",
    "\n",
    "for band in \"ugrizy\":\n",
    "    \n",
    "    # pull out the magnitudes and errors\n",
    "    mags = samples_w_errs.data[band].to_numpy()\n",
    "    errs = samples_w_errs.data[band + \"_err\"].to_numpy()\n",
    "    \n",
    "    # sort them by magnitude\n",
    "    mags, errs = mags[mags.argsort()], errs[mags.argsort()]\n",
    "    \n",
    "    # plot errs vs mags\n",
    "    ax.plot(mags, errs, label=band) \n",
    "    \n",
    "ax.legend()\n",
    "ax.set(xlabel=\"Magnitude (AB)\", ylabel=\"Error (mags)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the photometric error increases as magnitude gets dimmer, just like you would expect. Notice, however, that we have galaxies as dim as magnitude 30. This is because the Flow produces a sample much deeper than the LSST 5-sigma limiting magnitudes. There are no galaxies dimmer than magnitude 30 because LSSTErrorModel sets magnitudes > 30 equal to NaN (the default flag for non-detections)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"QuantityCut\"></a>\n",
    "## Degrader 2: QuantityCut\n",
    "\n",
    "Recall how the sample above has galaxies as dim as magnitude 30. This is well beyond the LSST 5-sigma limiting magnitudes, so it will be useful to apply cuts to the data to filter out these super-dim samples. We can apply these cuts using the `QuantityCut` degrader. This degrader will cut out any samples that do not pass all of the specified cuts.\n",
    "\n",
    "Let's make and run degraders that first adds photometric errors, then cuts at i<25.3, which is the LSST gold sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_cut = QuantityCut.make_stage(name='cuts', cuts={\"i\": 25.3})                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can stick this into a Creator and draw a new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gold_w_errs = gold_cut(samples_w_errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the i column, you will see there are no longer any samples with i > 25.3. The number of galaxies returned has been nearly cut in half from the input sample and, unlike the LSSTErrorModel degrader, is not equal to the number of input objects.  Users should note that with degraders that remove galaxies from the sample the size of the output sample will not equal that of the input sample.\n",
    "\n",
    "One more note: it is easy to use the QuantityCut degrader as a SNR cut on the magnitudes. The magnitude equation is $m = -2.5 \\log(f)$. Taking the derivative, we have\n",
    "$$\n",
    "dm = \\frac{2.5}{\\ln(10)} \\frac{df}{f} = \\frac{2.5}{\\ln(10)} \\frac{1}{\\mathrm{SNR}}.\n",
    "$$\n",
    "So if you want to make a cut on galaxies above a certain SNR, you can make a cut\n",
    "$$\n",
    "dm < \\frac{2.5}{\\ln(10)} \\frac{1}{\\mathrm{SNR}}.\n",
    "$$\n",
    "For example, an SNR cut on the i band would look like this: `QuantityCut({\"i_err\": 2.5/np.log(10) * 1/SNR})`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"InvRedshiftIncompleteness\"></a>\n",
    "## Degrader 3: InvRedshiftIncompleteness\n",
    "\n",
    "Next, we will demonstrate the `InvRedshiftIncompleteness` degrader. It applies a selection function, which keeps galaxies with probability $p_{\\text{keep}}(z) = \\min(1, \\frac{z_p}{z})$, where $z_p$ is the ''pivot'' redshift. We'll use $z_p = 0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_incomplete = InvRedshiftIncompleteness.make_stage(name='incompleteness', pivot_redshift=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_incomplete_gold_w_errs = inv_incomplete(samples_gold_w_errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the redshift distributions of the samples we have generated so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4), dpi=100)\n",
    "\n",
    "zmin = 0\n",
    "zmax = 2.5\n",
    "\n",
    "hist_settings = {\n",
    "    \"bins\": 50,\n",
    "    \"range\": (zmin, zmax),\n",
    "    \"density\": True,\n",
    "    \"histtype\": \"step\",\n",
    "}\n",
    "\n",
    "ax.hist(samples_truth()[\"redshift\"], label=\"Truth\", **hist_settings)\n",
    "ax.hist(samples_gold_w_errs()[\"redshift\"], label=\"Gold\", **hist_settings)\n",
    "ax.hist(samples_incomplete_gold_w_errs()[\"redshift\"], label=\"Incomplete Gold\", **hist_settings)\n",
    "ax.legend(title=\"Sample\")\n",
    "ax.set(xlim=(zmin, zmax), xlabel=\"Redshift\", ylabel=\"Galaxy density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the Gold sample has significantly fewer high-redshift galaxies than the truth. This is because many of the high-redshift galaxies have i > 25.3.\n",
    "\n",
    "You can further see that the Incomplete Gold sample has even fewer high-redshift galaxies. This is exactly what we expected from this degrader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"LineConfusion\"></a>\n",
    "## Degrader 4: LineConfusion\n",
    "\n",
    "`LineConfusion` is a degrader that simulates spectroscopic errors resulting from the confusion of different emission lines.\n",
    "\n",
    "For this example, let's use the degrader to simulate a scenario in which which 2% of [OII] lines are mistaken as [OIII] lines, and 1% of [OIII] lines are mistaken as [OII] lines. (note I do not know how realistic this scenario is!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OII = 3727\n",
    "OIII = 5007\n",
    "\n",
    "lc_2p_0II_0III = LineConfusion.make_stage(name='lc_2p_0II_0III',\n",
    "                                          true_wavelen=OII, wrong_wavelen=OIII, frac_wrong=0.02)\n",
    "lc_1p_0III_0II = LineConfusion.make_stage(name='lc_1p_0III_0II',\n",
    "                                          true_wavelen=OIII, wrong_wavelen=OII, frac_wrong=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_conf_inc_gold_w_errs = lc_1p_0III_0II(lc_2p_0II_0III(samples_incomplete_gold_w_errs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the redshift distributions one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4), dpi=100)\n",
    "\n",
    "zmin = 0\n",
    "zmax = 2.5\n",
    "\n",
    "hist_settings = {\n",
    "    \"bins\": 50,\n",
    "    \"range\": (zmin, zmax),\n",
    "    \"density\": True,\n",
    "    \"histtype\": \"step\",\n",
    "}\n",
    "\n",
    "ax.hist(samples_truth()[\"redshift\"], label=\"Truth\", **hist_settings)\n",
    "ax.hist(samples_gold_w_errs()[\"redshift\"], label=\"Gold\", **hist_settings)\n",
    "ax.hist(samples_incomplete_gold_w_errs()[\"redshift\"], label=\"Incomplete Gold\", **hist_settings)\n",
    "ax.hist(samples_conf_inc_gold_w_errs()[\"redshift\"], label=\"Confused Incomplete Gold\", **hist_settings)\n",
    "ax.legend(title=\"Sample\")\n",
    "ax.set(xlim=(zmin, zmax), xlabel=\"Redshift\", ylabel=\"Galaxy density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the redshift distribution of this new sample is essentially identical to the Incomplete Gold sample, with small perturbations that result from the line confusion. \n",
    "\n",
    "However the real impact of this degrader isn't on the redshift distribution, but rather that it introduces erroneous spec-z's into the photo-z training sets! To see the impact of this effect, let's plot the true spec-z's as present in the Incomplete Gold sample, vs the spec-z's listed in the new sample with Oxygen Line Confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6), dpi=85)\n",
    "\n",
    "ax.scatter(samples_incomplete_gold_w_errs()[\"redshift\"], samples_conf_inc_gold_w_errs()[\"redshift\"], \n",
    "           marker=\".\", s=1)\n",
    "\n",
    "ax.set(\n",
    "    xlim=(0, 2.5), ylim=(0, 2.5),\n",
    "    xlabel=\"True spec-z (in Incomplete Gold sample)\",\n",
    "    ylabel=\"Spec-z listed in the Confused sample\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clearly see the spec-z errors! The galaxies above the line y=x are the [OII] -> [OIII] galaxies, while the ones below are the [OIII] -> [OII] galaxies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "1cc5b75da6b94fd59f6c7b0992047078a9372d2242c3f23a554e39af5a039534"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
