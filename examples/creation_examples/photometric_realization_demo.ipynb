{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Engines and Degraders to Generate Galaxy Samples with Errors and Biases\n",
    "\n",
    "author: John Franklin Crenshaw, Sam Schmidt, Eric Charles, Ziang Yan\n",
    "\n",
    "last run successfully: April 26, 2023\n",
    "\n",
    "This notebook demonstrates how to do photometric realization from different magnitude error models. For more completed degrader demo, see `degradation-demo.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pzflow.examples import get_example_flow\n",
    "from rail.creation.engines.flowEngine import FlowCreator\n",
    "from rail.creation.degradation import (\n",
    "    LSSTErrorModel,\n",
    ")\n",
    "from rail.core.stage import RailStage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the path to the pretrained 'pzflow' used to generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pzflow\n",
    "import os\n",
    "flow_file = os.path.join(os.path.dirname(pzflow.__file__), 'example_files', 'example-flow.pzflow.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by setting up the Rail data store.  RAIL uses [ceci](https://github.com/LSSTDESC/ceci), which is designed for pipelines rather than interactive notebooks, the data store will work around that and enable us to use data interactively.  See the `rail/examples/goldenspike/goldenspike.ipynb` example notebook for more details on the Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"True\" Engine\n",
    "\n",
    "First, let's make an Engine that has no degradation. We can use it to generate a \"true\" sample, to which we can compare all the degraded samples below.\n",
    "\n",
    "Note: in this example, we will use a normalizing flow engine from the [pzflow](https://github.com/jfcrenshaw/pzflow) package. However, everything in this notebook is totally agnostic to what the underlying engine is.\n",
    "\n",
    "The Engine is a type of RailStage object, so we can make one using the `RailStage.make_stage` function for the class of Engine that we want.  We then pass in the configuration parameters as arguments to `make_stage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = int(1e5)\n",
    "flowEngine_truth = FlowCreator.make_stage(name='truth', model=flow_file, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check that the Engine correctly read the underlying PZ Flow object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowEngine_truth.get_data('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we invoke the `sample` method to generate some samples\n",
    "\n",
    "Note that this will return a `DataHandle` object, which can keep both the data itself, and also the path to where the data is written.  When talking to rail stages we can use this as though it were the underlying data and pass it as an argument.  This allows the rail stages to keep track of where their inputs are coming from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate magnitude error for extended sources, we need the information about major and minor axes of each galaxy. Here we simply generate random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_truth = flowEngine_truth.sample(n_samples, seed=0)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "samples_truth.data['major'] = np.abs(np.random.normal(loc=0.01, scale=0.1, size=n_samples)) # add major and minor axes\n",
    "b_to_a = 1 - 0.5*np.random.rand(n_samples)\n",
    "samples_truth.data['minor'] = samples_truth.data['major'] * b_to_a \n",
    "\n",
    "print(samples_truth())\n",
    "print(\"Data was written to \", samples_truth.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"LSSTErrorModel\"></a>\n",
    "LSSTErrorModel\n",
    "\n",
    "Now, we will demonstrate the `LSSTErrorModel`, which adds photometric errors using a model similar to the model from [Ivezic et al. 2019](https://arxiv.org/abs/0805.2366) (specifically, it uses the model from this paper, without making the high SNR assumption. To restore this assumption and therefore use the exact model from the paper, set `highSNR=True`.)\n",
    "\n",
    "Let's create an error model with the default settings for point sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorModel = LSSTErrorModel.make_stage(name='error_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the details of the model, including the default settings we are using, you can just print the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extended sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorModel_auto = LSSTErrorModel.make_stage(name='error_model_auto',\n",
    "                                                errortype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorModel_gaap = LSSTErrorModel.make_stage(name='error_model_gaap',\n",
    "                                                errortype=\"gaap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add this error model as a degrader and draw some samples with photometric errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_w_errs = errorModel(samples_truth)\n",
    "samples_w_errs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_w_errs_gaap = errorModel_gaap(samples_truth)\n",
    "samples_w_errs_gaap.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_w_errs_auto = errorModel_auto(samples_truth)\n",
    "samples_w_errs_auto.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice some of the magnitudes are NaN's. These are non-detections. This means those observed magnitudes were beyond the 30mag limit that is default in `LSSTErrorModel`. \n",
    "You can change this limit and the corresponding flag by setting `magLim=...` and `ndFlag=...` in the constructor for `LSSTErrorModel`. \n",
    "\n",
    "Let's plot the error as a function of magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes_ = plt.subplots(ncols=3, nrows=2, figsize=(15, 9), dpi=100)\n",
    "axes = axes_.reshape(-1)\n",
    "for i, band in enumerate(\"ugrizy\"):\n",
    "    ax = axes[i]\n",
    "    # pull out the magnitudes and errors\n",
    "    mags = samples_w_errs.data[band].to_numpy()\n",
    "    errs = samples_w_errs.data[band + \"_err\"].to_numpy()\n",
    "    \n",
    "    # sort them by magnitude\n",
    "    mags, errs = mags[mags.argsort()], errs[mags.argsort()]\n",
    "    \n",
    "    # plot errs vs mags\n",
    "    #ax.plot(mags, errs, label=band) \n",
    "    \n",
    "    #plt.plot(mags, errs, c='C'+str(i))\n",
    "    ax.scatter(samples_w_errs_gaap.data[band].to_numpy(),\n",
    "            samples_w_errs_gaap.data[band + \"_err\"].to_numpy(),\n",
    "                s=5, marker='.', color='C0', alpha=0.8, label='GAAP')\n",
    "    \n",
    "    ax.plot(mags, errs, color='C3', label='Point source')\n",
    "    \n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xlim(18, 31)\n",
    "    ax.set_ylim(-0.1, 3.5)\n",
    "    ax.set(xlabel=band+\" Band Magnitude (AB)\", ylabel=\"Error (mags)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes_ = plt.subplots(ncols=3, nrows=2, figsize=(15, 9), dpi=100)\n",
    "axes = axes_.reshape(-1)\n",
    "for i, band in enumerate(\"ugrizy\"):\n",
    "    ax = axes[i]\n",
    "    # pull out the magnitudes and errors\n",
    "    mags = samples_w_errs.data[band].to_numpy()\n",
    "    errs = samples_w_errs.data[band + \"_err\"].to_numpy()\n",
    "    \n",
    "    # sort them by magnitude\n",
    "    mags, errs = mags[mags.argsort()], errs[mags.argsort()]\n",
    "    \n",
    "    # plot errs vs mags\n",
    "    #ax.plot(mags, errs, label=band) \n",
    "    \n",
    "    #plt.plot(mags, errs, c='C'+str(i))\n",
    "    ax.scatter(samples_w_errs_auto.data[band].to_numpy(),\n",
    "            samples_w_errs_auto.data[band + \"_err\"].to_numpy(),\n",
    "                s=5, marker='.', color='C0', alpha=0.8, label='AUTO')\n",
    "    \n",
    "    ax.plot(mags, errs, color='C3', label='Point source')\n",
    "    \n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xlim(18, 31)\n",
    "    ax.set_ylim(-0.1, 3.5)\n",
    "    ax.set(xlabel=band+\" Band Magnitude (AB)\", ylabel=\"Error (mags)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the photometric error increases as magnitude gets dimmer, just like you would expect. Notice, however, that we have galaxies as dim as magnitude 30. This is because the Flow produces a sample much deeper than the LSST 5-sigma limiting magnitudes. There are no galaxies dimmer than magnitude 30 because LSSTErrorModel sets magnitudes > 30 equal to NaN (the default flag for non-detections).\n",
    "\n",
    "Also, you can find the GAaP and AUTO magnitude error are scattered due to variable galaxy sizes. Also, you can find that there are gaps between GAAP magnitude error and point souce magnitude error, this is because the additional factors due to aperture sizes have a minimum value of $\\sqrt{(\\sigma^2+A_{\\mathrm{min}})/\\sigma^2}$, where $\\sigma$ is the width of the beam, $A_{\\min}$ is an offset of the aperture sizes (taken to be 0.7 arcmin here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33169e9e67880b1853f4eac3eefbf27d2cf9aef466064efe02e233d73165a438"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
