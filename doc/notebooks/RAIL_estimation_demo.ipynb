{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.insert(0,\"/Users/sam/WORK/lib/python3.7/site-packages/rail-0.1.dev0-py3.7.egg\")\n",
    "#sys.path.insert(0,\"/Users/sam/WORK/lib/python3.7/site-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAIL/estimation Tutorial Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rail\n",
    "from rail.fileIO import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importing RAIL you should see a list of the available photo-z algorithms, as printed out above.  These are the names of the specific subclasses that invoke a particular method, and they are stored in the `rail.estimation.algos` subdirectory of RAIL.<br>\n",
    "\n",
    "`randomPZ` is a very simple class that does not actually predict a meaningful photo-z, instead it produces a randomly drawn Gaussian for each galaxy.<br>\n",
    "`trainZ` is our \"pathological\" estimator, it makes a PDF from a histogram of the training data and assigns that PDF to every galaxy.<br>\n",
    "`simpleNN` uses `sklearn`'s neural network to predict a point redshift from the training data, the assigns a sigma width based on the redshift for another toy model example<br>\n",
    "`FZBoost` is a fully functional photo-z algorith, implementing the FlexZBoost conditional density estimate method that was used in the PhotoZDC1 paper.<br>\n",
    "\n",
    "RAIL/estimation is set up so that parameters for general setup (location of data and some settings) are stored in a local yaml file, while settings for a specific code can either be stored in a yaml file or a dictionary.<br>\n",
    "We will use the yaml file stored in the current directory called `example_estimation_base.yaml` which contains the following entries:<br>\n",
    "```\n",
    "base_config:\n",
    "  trainfile: ./Packages/RAIL/tests/data/test_dc2_training_9816.hdf5\n",
    "  testfile: ./Packages/RAIL/tests/data/test_dc2_validation_9816.hdf5\n",
    "  hdf5_groupname: photometry\n",
    "  chunk_size: 2500\n",
    "  configpath: ./configs\n",
    "  outpath: ./results\n",
    "  output_format: old\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yaml file will be opened when a RAIL.estimation.Estimator class is invoked.  The quantities in the yaml file are:<br><br>\n",
    "-trainfile (str): path to the training data for this run<br>\n",
    "-testfile (str): path to the testing data for this run<br>\n",
    "-hdf5_groupname (str): the toplevel `groupname` for the hdf5 file, if the data is stored in the input hdf5 under e.g. `photometry`<br>\n",
    "-chunk_size (int): `RAIL.fileIO` has an iterator method that can break the data in chunks of size `chunk_size` rather than run all at once<br>\n",
    "-configpath (str): location of config files\n",
    "-outpath (str): directory in which to write output files\n",
    "-output_format(str): If `output_format` is set to \"qp\" the method will return PDF data in qp format, if set to any other value it will return a dictionary.  We will demonstrate both data types in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code specific parameters are stored in a dictionary.  This dictionary should itself contain a dictionary named `run_params`.  Let's start with a very simple demonstration using `simpleNN`.  `simpleNN` is just `sklearn`'s neural net method set up within the RAIL interface.  It estimates a point estimate redshift for each galaxy, then sets the width of the PDF based purely on the redshift.  This is a toy model estimator, but nicely illustrates some properties of RAIL. The parameters we'll use for simpleNN are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dict = {'run_params': {\n",
    "  'class_name': 'simpleNN',\n",
    "  'run_name': 'test_simpleNN',\n",
    "  'zmin': 0.0,\n",
    "  'zmax': 3.0,\n",
    "  'nzbins': 301,\n",
    "  'width': 0.05,\n",
    "  'inform_options': {'save_train': True, 'load_model': False, 'modelfile': 'demo_NN_model.pkl'}\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these parameters, `zmin`, `zmax`, and `nzbins` control the gridded parameterization on wich the PDF will be output and stored (if not using `qp` format, in which case these parameters are ignored).<br>\n",
    "`width` sets the scale of the Gaussian width assumed for the PDFs.<br>\n",
    "`inform_options` is a dictionary that stores some information pertaining to the training process: <br>\n",
    "`modelfile` is a string argument that stores a filename.<br>    \n",
    "`save_train` is a boolean flag that if set to True will save the trained model to the filename stored in `modelfile` for later import.<br>\n",
    "`load_model` is a boolean flag that if set to true, will attempt to load a pretrained model from the filename in `modelfile`<br>\n",
    "\n",
    "We will see example uses of `save_train` and `load_model` later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a rail object with a call to the base class, in this case that is `rail.estimation.algos.flexzboost.FZBoost`<br>\n",
    "\n",
    "You can also find the specific class name from the name of the algorithm with:<br>\n",
    "```\n",
    "classname = 'simpleNN'\n",
    "code = Estimator._find_subclass(classname)\n",
    "```\n",
    "and then instantiate with \n",
    "```\n",
    "pz = code('example_estimation_base.yaml', nn_dict)  \n",
    "```\n",
    "We will hardcode here for the concrete example. The `simpleNN` class is in the file `rail/estimation/algos/sklearn_nn.py`, so to create an instance of this class we need a call to `rail.estimation.algos.sklearn_nn.simpleNN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pz = rail.estimation.algos.sklearn_nn.simpleNN('example_estimation_base.yaml',nn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load our training data, which is stored in hdf5 format.  We'll load it with the fileIO function load_training_data(), which returns a dictionary containing numpy arrays of all the columns in the hdf5 file, which matches the input format expected by the rail estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = pz.trainfile\n",
    "train_fmt = 'hdf5'\n",
    "training_data = load_training_data(trainfile, train_fmt, pz.groupname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to train the neural net, which is done with the `inform` function present in every RAIL/estimation code. In `nn_dict` we have the `inform_options[save_train]` option set to `True`, so the trained model object will be saved in pickle format to the filename specified in `inform_options[modelfile]`, in this case `demo_NN_model.pkl`.  In the future, rather than re-run a potentially time consuming training set, we can simply load this pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pz.inform(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run our algorithm on the data to produce simple photo-z estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_training_data(pz.testfile, 'hdf5', 'photometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = pz.estimate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output file is a dictionary containing the redshift PDFs and the mode of the PDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the redshift mode against the true redshifts to see how they look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(test_data['redshift'],results_dict['zmode'],s=1,c='k',label='simple NN mode')\n",
    "plt.plot([0,3],[0,3],'r--');\n",
    "plt.xlabel(\"true redshift\")\n",
    "plt.ylabel(\"simple NN photo-z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, given our very simple estimator.  For the PDFs, the simpleNN is storing a gridded parameterization where the PDF is evaluated at a fixed set of redshifts for each galaxy.  That grid is stored in `pz.zgrid`, and we'll need that to plot.  Remember that our simple Neural Net just estimated a point photo-z then assumed a Gaussian, so all PDFs will be of that simple form.  Let's plot an example pdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galid = 9529\n",
    "zgrid = pz.zgrid\n",
    "single_gal = results_dict['pz_pdf'][galid]\n",
    "single_zmode = results_dict['zmode'][galid]\n",
    "truez = test_data['redshift'][galid]\n",
    "plt.plot(zgrid,single_gal,color='k',label='single pdf')\n",
    "plt.axvline(single_zmode,color='k',label='mode')\n",
    "plt.axvline(truez,color='r',label='true redshift')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"p(z)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORE HERE! SOME PLOTS, READ IN TRAINED MODEL AND SUCH ONCE I FIX PLOTTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That illustrates the basics, now let's use a more well developed method, FZBoost, and use the iterator utility from fileIO to evaluate data in chunks, and output in `qp` format.  We'll start with a dictionary of setup parameters for FZBoost, just as we had for simpleNN.  FZBoost does a more in depth training than simpleNN, and as such has more input parameters to control behavior.  A few important ones: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fz_dict = {'run_params': {\n",
    "  'class_name': 'FZBoost',\n",
    "  'run_name': 'test_FZBoost',\n",
    "  'zmin': 0.0,\n",
    "  'zmax': 3.0,\n",
    "  'nzbins': 301,\n",
    "  'trainfrac': 0.75,\n",
    "  'bumpmin': 0.02,\n",
    "  'bumpmax': 0.35,\n",
    "  'nbump': 20,\n",
    "  'sharpmin': 0.7,\n",
    "  'sharpmax': 2.1,\n",
    "  'nsharp': 15,\n",
    "  'max_basis': 35,\n",
    "  'basis_system': 'cosine',\n",
    "  'regression_params': {'max_depth': 8,'objective':'reg:squarederror'},\n",
    "  'inform_options': {'save_train': True, 'load_model': False, 'modelfile': 'demo_FZB_model.pkl'}\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also demonstrate using qp as our storage format.  We will do this by using `example_estimation_base_qp.yaml`, which has the same parameters as `example_estimation_base.yaml`, but with `output_format` set to `qp` instead of `old`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pzflex = rail.estimation.algos.flexzboost.FZBoost('example_estimation_base_qp.yaml',fz_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this data to train our model using `FZBoost`'s inform() method.  `FZBoost` uses xgboost to determine a conditional density estimate model, and also fits a `bump_thresh` parameter that erases small peaks that are an artifact of the `cosine` parameterization.  It then finds a best fit `sharpen` parameter that modulates the peaks with a power law.<br>\n",
    "We have `save_train` set to `True` in our `inform_options`, so this will save a pickled version of the best fit model to the file specified in `inform_options['modelfile']`, which is set above to `demo_FZB_model.pkl`.  We can use the same training data that we used for `simpleNN`.  `FZBoost` is a bit more sophisticated than `simpleNN`, so it will take a bit longer to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pzflex.inform(training_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took quite a while to train! But, because we had `inform_options[save_train]` set to `True` we have saved the pretrained model in the file `demo_FZB_model.pkl`.  To save time we can load this pickled model without having to repeat the training stage in future runs for this specific training data, and that should be much faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pzflex.load_pretrained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, under a second.  So, if you are running an algorithm with a burdensome training requirement, saving a trained copy of the model for later repeated use can be a real time saver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a temp qp append function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qpappend(ens1,ens2):\n",
    "    n1meta = ens1.metadata()\n",
    "    n2meta = ens1.metadata()\n",
    "    for key in n1meta.keys():\n",
    "        if isinstance(n1meta[key], np.ndarray):\n",
    "            if n1meta[key].any() != n2meta[key].any():\n",
    "                print(\"parameterizations differ!\")\n",
    "                exit(2)\n",
    "        else:\n",
    "            if n1meta[key] != n2meta[key]:\n",
    "                print(\"parameterizations differ!\")\n",
    "                exit(2)\n",
    "    n1data = ens1.objdata()\n",
    "    n2data = ens2.objdata()\n",
    "    for key in n1data.keys():\n",
    "        n1data[key] = np.vstack([n1data[key], n2data[key]])\n",
    "    ens1.update_objdata(n1data)\n",
    "    return ens1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute photo-z's using with the `estimate` method.  `rail.fileIO` has a function that will iterate through the data, named `iter_chunk_hdf5_data`.  This can be useful if you have a very large datafile with millions of galaxies and you do not want to store all of the data in memory at once, and e.g. want to write out the PDFs to file as you progress.  Our demo file has only ~20,000 galaxies, but we will use the iterator here as an example.  the `rail.filIO` function `iter_chunk_hdf5_data` takes arguments `infile` (path to the data file), `chunk_size` (how many galaxies you want to include in each iterator chunk), and `groupname` (the hdf5 groupname for the input data file).  We will use the same datafile used for our `simpleNN` demo.  Because we aren't writing to file, we will just stack the astropy tables returned by our iterator into one big qp file using the custom `qpappend` function defined in the cell above.  Again, if memory was a problem, you could instead write each file out to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "datafile =  \"Packages/RAIL/tests/data/test_dc2_validation_9816.hdf5\"\n",
    "for chunk, (start, end, data) in enumerate(iter_chunk_hdf5_data(pz.testfile,pz._chunk_size,pz.hdf5_groupname)):\n",
    "    print(f\"calculating pdfs[{start}:{end}]\")\n",
    "    pz_data_chunk = pzflex.estimate(data)\n",
    "    if chunk == 0:\n",
    "        FZ_pdfs = pz_data_chunk\n",
    "    else:\n",
    "        FZ_pdfs = qpappend(FZ_pdfs, pz_data_chunk)\n",
    "        del pz_data_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = FZ_pdfs.objdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alldata['yvals'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = qp.plotting.plot_native(FZ_pdfs[1225], xlim=(0.,3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
